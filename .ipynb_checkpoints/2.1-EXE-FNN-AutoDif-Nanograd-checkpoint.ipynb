{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAva8TnYFtFu"
   },
   "source": [
    "# Contents and why we need this lab\n",
    "\n",
    "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
    "\n",
    "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCa7HzwpFtFy"
   },
   "source": [
    "# External sources of information\n",
    "\n",
    "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
    "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
    "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjiIp-TFtF0"
   },
   "source": [
    "# This notebook will follow the next steps:\n",
    "\n",
    "1. Nanograd automatic differentiation framework\n",
    "2. Finite difference method\n",
    "3. Data generation\n",
    "4. Defining and initializing the network\n",
    "5. Forward pass\n",
    "6. Training loop \n",
    "7. Testing your model\n",
    "8. Further extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyXeAA-HuT7s"
   },
   "source": [
    "# Nanograd automatic differention framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6UWKCLKubgA"
   },
   "source": [
    "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jd4CoEBNzNWS"
   },
   "outputs": [],
   "source": [
    "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
    "\n",
    "from math import exp, log\n",
    "\n",
    "class Var:\n",
    "    \"\"\"\n",
    "    A variable which holds a float and enables gradient computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\n",
    "        assert type(val) == float\n",
    "        self.v = val\n",
    "        self.grad_fn = grad_fn\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def backprop(self, bp):\n",
    "        self.grad += bp\n",
    "        for input, grad in self.grad_fn():\n",
    "            input.backprop(grad * bp)\n",
    "\n",
    "    def backward(self):\n",
    "        self.backprop(1.0)\n",
    "\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
    "\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
    "\n",
    "    def __neg__(self: 'Var') -> 'Var':\n",
    "        return Var(-1.0) * self\n",
    "\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
    "\n",
    "    def relu(self):\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDX67D6jzcte"
   },
   "source": [
    "A few examples illustrate how we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xk6PeLc3zwPT",
    "outputId": "47e431b2-07ba-4cb1-ea21-997769641c67"
   },
   "outputs": [],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "f = a * b\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for v in [a, b, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmKhYgsY0g_o",
    "outputId": "06c1b1df-c33c-40d3-922a-624612a591c7"
   },
   "outputs": [],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "c = a * b\n",
    "d = Var(9.0)\n",
    "e = a * d\n",
    "f = c + e\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe3B6uEH140p"
   },
   "source": [
    "## Exercise a) What is being calculated?\n",
    "\n",
    "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer a)\n",
    "\n",
    "The output is the gradient of specific variable, like the output of f.backward(), output a is Var(v=3, grad=14), it means the value of a is 3 and the differential coefficient of a to f is 14.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial a} = 14\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8_Q0t2I3Ruj"
   },
   "source": [
    "## Exercise b) How does the backward function work?\n",
    "\n",
    "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
    "\n",
    "Go through the following four steps and answer the questions on the way:\n",
    "\n",
    "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
    "\n",
    "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
    "\n",
    "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. \n",
    "\n",
    "4. Write down the sequence of calls to backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idGr71jYXl26"
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "import graphviz\n",
    "\n",
    "#logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
    "\n",
    "#graphviz.__version__, graphviz.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer b)\n",
    "\n",
    "As shown before, a=3, b=5, d=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "KPe30Q2QXzeG",
    "outputId": "7fa002cd-a018-4dbb-ddf1-28ed5e99ee19"
   },
   "outputs": [],
   "source": [
    "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
    "\n",
    "e1.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "e1.attr('node', shape='circle')\n",
    "e1.edge('a', 'f', label='df/da=5')\n",
    "e1.edge('b', 'f', label='df/db=3')\n",
    "\n",
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "0nittR-mZFeX",
    "outputId": "fa3656a3-732c-4abe-8084-98a492b0d6be"
   },
   "outputs": [],
   "source": [
    "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
    "\n",
    "e2.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "e2.attr('node', shape='circle')\n",
    "e2.edge('a', 'c', label='dc/da=5')\n",
    "e2.edge('b', 'c', label='dc/db=3')\n",
    "e2.edge('a', 'e', label='de/da=9')\n",
    "e2.edge('d', 'e', label='de/dd=3')\n",
    "e2.edge('c', 'f', label='df/dc=1')\n",
    "e2.edge('e', 'f', label='df/de=1')\n",
    "\n",
    "e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5oi21W4gpeM"
   },
   "source": [
    "## Exercise c) What happens if we run backward again?\n",
    "\n",
    "Try to execute the code below. Explain what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCtpJyr-gyX1",
    "outputId": "d014bcfa-c9ae-49c3-d268-91cc6ca94ea5"
   },
   "outputs": [],
   "source": [
    "f.backward()\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer c)\n",
    "\n",
    "The gradient of each variable is doubled. Because in this class, the author create one array to store each variable's value and gradient, what's more, the overload operators like: add and so on, are defined with corresponding derivate formula.\n",
    "\n",
    "Therefore, the intial gradient of each variable is the value calculated before, then if f.backward() twice, the gradient will be calculated again, ie, double. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8bPVq2VhsP-"
   },
   "source": [
    "## Exercise d) Zero gradient\n",
    "\n",
    "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnyPDQx9lJe0",
    "outputId": "7a125fdc-60c4-4340-a580-8b82aea5b0db"
   },
   "outputs": [],
   "source": [
    "a = Var(2.0)\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)\n",
    "\n",
    "f.backprop(-1.0)\n",
    "\n",
    "print(\"_________________________________________________\")\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer d)\n",
    "\n",
    "For a, it mainly because the property of jupyter notebook. If we define the same variable with different value, the old one still exist and the related functions or algorithms still based on the old one, they will not be updated with new one. Therefore, the \"a\" in this exercise is the new \"a\", it does not one part of \"f\" function. That is why the gradient is zero.\n",
    "\n",
    "for f.backprop(-1), backprop() is different with backward(), in backward() the initial value of backprop() is 1. Therefore that's why if we f.backward() twice the gradient of each value will be double. But if we specify the value of f.backprop() is \"-1\" and run it, the doubled gradient will decrease their original gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4057_ljNvWB"
   },
   "source": [
    "## Exercise e) Test correctness of derivatives with the finite difference method\n",
    "\n",
    "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
    "$$\n",
    "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
    "$$\n",
    "for $da \\ll 1$.\n",
    "\n",
    "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TGil92lSXDN",
    "outputId": "7ef5489b-b525-4132-ab08-0b1109c07f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=5.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "8.000000661922968\n"
     ]
    }
   ],
   "source": [
    "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
    "def f_function(a):\n",
    "    a = Var(a)\n",
    "    b = Var(5.0)\n",
    "    f = a * b\n",
    "    f.backward()\n",
    "    \n",
    "    return a,b,f\n",
    "\n",
    "for v in f_function(3.0):\n",
    "    print(v)\n",
    "\n",
    "# Insert your finite difference code here\n",
    "def finite_difference(da=1e-10):\n",
    "    \"\"\"\n",
    "    This function compute the finite difference between\n",
    "    \n",
    "    Input:\n",
    "    da:          The finite difference                           (float)\n",
    "    \n",
    "    Output:\n",
    "    finite_difference: numerical approximation to the derivative (float) \n",
    "    \"\"\"\n",
    "    \n",
    "    fa_da = (4+da)**2           # <- Insert correct expression\n",
    "    fa = 4 ** 2               # <- Insert correct expression\n",
    "\n",
    "    finite_difference = (fa_da - fa) / da\n",
    "    \n",
    "    return finite_difference\n",
    "\n",
    "print(finite_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pZar5RKaUkg"
   },
   "source": [
    "# Create an artificial dataset to play with\n",
    "\n",
    "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y6yfMAQ8aduj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4YabfD43ajNh"
   },
   "outputs": [],
   "source": [
    "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
    "    # Create covariates and response variable\n",
    "    if D1:\n",
    "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
    "        np.random.shuffle(X)\n",
    "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
    "    else:\n",
    "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
    "        np.random.shuffle(X)    \n",
    "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
    "\n",
    "    # Stack them together vertically to split data set\n",
    "    data_set = np.vstack((X.T,y)).T\n",
    "    \n",
    "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
    "    \n",
    "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
    "    train_mu = np.mean(train, axis=0)\n",
    "    train_sigma = np.std(train, axis=0)\n",
    "    \n",
    "    train = (train-train_mu)/train_sigma\n",
    "    validation = (validation-train_mu)/train_sigma\n",
    "    test = (test-train_mu)/train_sigma\n",
    "    \n",
    "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
    "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
    "\n",
    "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u1oDngHLapIz"
   },
   "outputs": [],
   "source": [
    "D1 = True\n",
    "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Ysfa3FsBavlm",
    "outputId": "399e5382-ae7d-48f6-9774-7ea4c73e7d95"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if D1:\n",
    "    plt.scatter(x_train[:,0], y_train);\n",
    "    plt.scatter(x_validation[:,0], y_validation);\n",
    "    plt.scatter(x_test[:,0], y_test);\n",
    "else:\n",
    "    plt.scatter(x_train[:,1], y_train);\n",
    "    plt.scatter(x_validation[:,1], y_validation);\n",
    "    plt.scatter(x_test[:,1], y_test);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zac2HHNlgbpm"
   },
   "outputs": [],
   "source": [
    "# convert from nparray to Var\n",
    "def nparray_to_Var(x):\n",
    "    if x.ndim==1:\n",
    "        y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
    "        \n",
    "    else:\n",
    "        y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
    "        \n",
    "    return y\n",
    "   \n",
    "x_train = nparray_to_Var(x_train)\n",
    "y_train = nparray_to_Var(y_train)\n",
    "x_validation = nparray_to_Var(x_validation)\n",
    "y_validation = nparray_to_Var(y_validation)\n",
    "x_test = nparray_to_Var(x_test)\n",
    "y_test = nparray_to_Var(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbjrqcpVFtGe"
   },
   "source": [
    "# Defining and initializing the network\n",
    "\n",
    "The steps to create a feed forward neural network are the following:\n",
    "\n",
    "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
    "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
    "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
    "\n",
    "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
    "\n",
    "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ij_ieRsAt7Xt"
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def init_bias(self, n_out):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eb18N5phuIha"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NormalInitializer(Initializer):\n",
    "\n",
    "    def __init__(self, mean=0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "    def init_bias(self, n_out):\n",
    "        return [Var(0.0) for _ in range(n_out)]\n",
    "\n",
    "class ConstantInitializer(Initializer):\n",
    "\n",
    "    def __init__(self, weight=1.0, bias=0.0):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "    def init_bias(self, n_out):\n",
    "        return [Var(self.bias) for _ in range(n_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jOLYGnZKuM6W"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\n",
    "        self.bias = initializer.init_bias(n_out)\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
    "                      \n",
    "    def parameters(self) -> Sequence[Var]:\n",
    "        params = []\n",
    "        for r in self.weights:\n",
    "            params += r\n",
    "        \n",
    "        return params + self.bias\n",
    "\n",
    "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
    "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
    "        # to the current layer matches the number of nodes in the current layer\n",
    "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
    "        weights = self.weights\n",
    "        out = []\n",
    "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
    "        # We therefore loop over the (number of) nodes in the current layer:\n",
    "        for j in range(len(weights[0])): \n",
    "            # Initialize the node value depending on its corresponding parameters.\n",
    "            node = Var(0.0) # <- Insert code\n",
    "            \n",
    "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
    "            for i in range(len(weights)):\n",
    "                node += weights[i][j] * single_input[i] + self.bias[j]# <- Insert code\n",
    "                \n",
    "            node = self.act_fn(node)\n",
    "            out.append(node)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpIZPBpNI0pO"
   },
   "source": [
    "## Exercise f) Add more activation functions\n",
    "\n",
    "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
    " \n",
    "Implement the following activation functions in the Var class:\n",
    "\n",
    "* Identity: $$\\mathrm{identity}(x) = x$$\n",
    "* Hyperbolic tangent: $$\\tanh(x)$$\n",
    "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
    "\n",
    "Hint: You can seek inspiration in the relu method in the Var class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, log\n",
    "\n",
    "class Var:\n",
    "    \"\"\"\n",
    "    A variable which holds a float and enables gradient computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\n",
    "        assert type(val) == float\n",
    "        self.v = val\n",
    "        self.grad_fn = grad_fn\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def backprop(self, bp):\n",
    "        self.grad += bp\n",
    "        for input, grad in self.grad_fn():\n",
    "            input.backprop(grad * bp)\n",
    "\n",
    "    def backward(self):\n",
    "        self.backprop(1.0)\n",
    "\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
    "\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
    "\n",
    "    def __neg__(self: 'Var') -> 'Var':\n",
    "        return Var(-1.0) * self\n",
    "\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
    "\n",
    "    def relu(self):\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
    "    \n",
    "    def identity(self):\n",
    "        return Var(self.v, lambda:[(self, 1.0)])\n",
    "\n",
    "    def tanh(self):\n",
    "        return Var(self.v, lambda:[(self, 1.0-(self.v)**2)])\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return Var(self.v, lambda:[(self, self.v * (1-self.v))])\n",
    "    \n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "    \n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_8n_SKnIW2F"
   },
   "source": [
    "## Exercise g) Complete the forward pass\n",
    "\n",
    "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "xDEjtePxE7Mv",
    "outputId": "753406cd-d8a1-4282-ce03-25ad959b0e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Var(v=-0.0099, grad=0.0000)], [Var(v=-0.0199, grad=0.0000)], [Var(v=-0.0018, grad=0.0000)], [Var(v=-0.0140, grad=0.0000)], [Var(v=-0.0147, grad=0.0000)], [Var(v=-0.0135, grad=0.0000)], [Var(v=-0.0088, grad=0.0000)], [Var(v=-0.0034, grad=0.0000)], [Var(v=-0.0144, grad=0.0000)], [Var(v=-0.0055, grad=0.0000)], [Var(v=-0.0153, grad=0.0000)], [Var(v=-0.0089, grad=0.0000)], [Var(v=-0.0123, grad=0.0000)], [Var(v=-0.0102, grad=0.0000)], [Var(v=-0.0066, grad=0.0000)], [Var(v=-0.0128, grad=0.0000)], [Var(v=-0.0235, grad=0.0000)], [Var(v=-0.0112, grad=0.0000)], [Var(v=-0.0012, grad=0.0000)], [Var(v=-0.0039, grad=0.0000)], [Var(v=-0.0066, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=-0.0172, grad=0.0000)], [Var(v=-0.0174, grad=0.0000)], [Var(v=-0.0039, grad=0.0000)], [Var(v=-0.0105, grad=0.0000)], [Var(v=-0.0223, grad=0.0000)], [Var(v=-0.0245, grad=0.0000)], [Var(v=-0.0026, grad=0.0000)], [Var(v=-0.0155, grad=0.0000)], [Var(v=-0.0131, grad=0.0000)], [Var(v=-0.0037, grad=0.0000)], [Var(v=-0.0114, grad=0.0000)], [Var(v=-0.0124, grad=0.0000)], [Var(v=-0.0118, grad=0.0000)], [Var(v=-0.0023, grad=0.0000)], [Var(v=-0.0142, grad=0.0000)], [Var(v=-0.0054, grad=0.0000)], [Var(v=-0.0100, grad=0.0000)], [Var(v=-0.0072, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.0069, grad=0.0000)], [Var(v=-0.0231, grad=0.0000)], [Var(v=-0.0086, grad=0.0000)], [Var(v=-0.0068, grad=0.0000)], [Var(v=-0.0082, grad=0.0000)], [Var(v=-0.0148, grad=0.0000)], [Var(v=-0.0140, grad=0.0000)], [Var(v=-0.0031, grad=0.0000)], [Var(v=-0.0143, grad=0.0000)], [Var(v=-0.0062, grad=0.0000)], [Var(v=-0.0005, grad=0.0000)], [Var(v=-0.0156, grad=0.0000)], [Var(v=-0.0070, grad=0.0000)], [Var(v=-0.0248, grad=0.0000)], [Var(v=-0.0218, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)], [Var(v=-0.0052, grad=0.0000)], [Var(v=-0.0082, grad=0.0000)], [Var(v=-0.0123, grad=0.0000)], [Var(v=-0.0145, grad=0.0000)], [Var(v=-0.0087, grad=0.0000)], [Var(v=-0.0143, grad=0.0000)], [Var(v=-0.0070, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.0117, grad=0.0000)], [Var(v=-0.0126, grad=0.0000)], [Var(v=-0.0183, grad=0.0000)], [Var(v=-0.0090, grad=0.0000)], [Var(v=-0.0072, grad=0.0000)], [Var(v=-0.0035, grad=0.0000)], [Var(v=-0.0016, grad=0.0000)], [Var(v=-0.0142, grad=0.0000)], [Var(v=-0.0083, grad=0.0000)], [Var(v=-0.0012, grad=0.0000)], [Var(v=-0.0224, grad=0.0000)], [Var(v=-0.0062, grad=0.0000)], [Var(v=-0.0060, grad=0.0000)], [Var(v=-0.0079, grad=0.0000)], [Var(v=-0.0234, grad=0.0000)], [Var(v=-0.0036, grad=0.0000)], [Var(v=-0.0021, grad=0.0000)], [Var(v=-0.0129, grad=0.0000)], [Var(v=-0.0018, grad=0.0000)], [Var(v=-0.0065, grad=0.0000)], [Var(v=-0.0028, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0073, grad=0.0000)], [Var(v=-0.0133, grad=0.0000)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0085, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0077, grad=0.0000)], [Var(v=-0.0132, grad=0.0000)], [Var(v=-0.0110, grad=0.0000)], [Var(v=-0.0132, grad=0.0000)], [Var(v=-0.0061, grad=0.0000)], [Var(v=-0.0010, grad=0.0000)], [Var(v=-0.0212, grad=0.0000)], [Var(v=-0.0078, grad=0.0000)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0076, grad=0.0000)], [Var(v=-0.0116, grad=0.0000)], [Var(v=-0.0067, grad=0.0000)], [Var(v=-0.0197, grad=0.0000)]]\n"
     ]
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "def forward(input, network):\n",
    "    \n",
    "    def forward_single(x, network):\n",
    "        for layer in network:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    output = [ forward_single(input[n], network) for n in range(len(input))]\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(forward(x_train, NN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLrGJytZFtGm"
   },
   "source": [
    "## Exercise h) Print all network parameters\n",
    "\n",
    "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iac-VwYGFtGm"
   },
   "outputs": [],
   "source": [
    "# Insert code here and in the DenseLayer class\n",
    "from typing import Sequence\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\n",
    "        self.bias = initializer.init_bias(n_out)\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return \"%s input nodes \\n%s output nodes \\n Weights:%s \\n Biases:%s\" % (len(self.weights), len(self.weights[0]), self.weights, self.bias)\n",
    "                      \n",
    "    def parameters(self) -> Sequence[Var]:\n",
    "        params = []\n",
    "        for r in self.weights:\n",
    "            params += r\n",
    "        \n",
    "        return params + self.bias\n",
    "\n",
    "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
    "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
    "        # to the current layer matches the number of nodes in the current layer\n",
    "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
    "        weights = self.weights\n",
    "        out = []\n",
    "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
    "        # We therefore loop over the (number of) nodes in the current layer:\n",
    "        for j in range(len(weights[0])): \n",
    "            # Initialize the node value depending on its corresponding parameters.\n",
    "            node = Var(0.0) # <- Insert code\n",
    "            \n",
    "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
    "            for i in range(len(weights)):\n",
    "                node += weights[i][j] * single_input[i] + self.bias[j]# <- Insert code\n",
    "                \n",
    "            node = self.act_fn(node)\n",
    "            out.append(node)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 input nodes \n",
      "5 output nodes \n",
      " Weights:[[Var(v=-0.0894, grad=0.0000), Var(v=-0.0105, grad=0.0000), Var(v=0.0186, grad=0.0000), Var(v=0.0363, grad=0.0000), Var(v=-0.1723, grad=0.0000)]] \n",
      " Biases:[Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n"
     ]
    }
   ],
   "source": [
    "print(DenseLayer(1, 5, lambda x: x.relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_79HOAXrFtHK"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Now that we have defined our activation functions we can visualize them to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1FcylHqLTl-Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26461988a60>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "# convert from Var to ndarray  \n",
    "def Var_to_nparray(x):\n",
    "    y = np.zeros((len(x),len(x[0])))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            y[i,j] = x[i][j].v\n",
    "    \n",
    "    return y\n",
    "\n",
    "# define 1-1 network with weight = 1 and relu activation \n",
    "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
    "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
    "\n",
    "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oOL2UolJFtHL"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing all activation layers\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "units = {\n",
    "    \"identity\": lambda x: x.identity(),\n",
    "    #\"sigmoid\": lambda x: x.sigmoid(),  <- uncomment before sharing\n",
    "    \"relu\": lambda x: x.relu(),\n",
    "    #\"tanh\": lambda x: x.tanh() <- uncomment before sharing\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Our activation functions', fontsize=20)\n",
    "plt.ylim([-2, 5])\n",
    "plt.xlim([-6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-jdEl-7FtGs"
   },
   "source": [
    "# Advanced initialization schemes\n",
    "\n",
    "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
    "\n",
    "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
    "\n",
    "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
    "\n",
    "The Glorot initialization has the form: \n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
    "\n",
    "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
    "\n",
    "The He initialization is very similar\n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqeyab9qFtGs"
   },
   "source": [
    "## Exercise i) Glorot and He initialization\n",
    " \n",
    "Using the Initializer class, implement functions that implement Glorot and He \n",
    "\n",
    "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
    "\n",
    "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qyk01CgaFtGt"
   },
   "outputs": [],
   "source": [
    "## Glorot\n",
    "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
    "    std = 2 / (n_in + n_out) # <- replace with proper initialization\n",
    "    return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
    "\n",
    "## He\n",
    "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
    "    std = 1 / (n_in) # <- replace with proper initialization\n",
    "    return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XyXBD37FtHk"
   },
   "source": [
    "## Exercise j) Forward pass unit test\n",
    "\n",
    "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
    "\n",
    "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "k0miqRUAFtHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26461982820>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Insert code here\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "x = nparray_to_Var(x)\n",
    "\n",
    "NN = [\n",
    "    DenseLayer_Glorot_tanh(1, 5),\n",
    "    DenseLayer_He_relu(5, 1)\n",
    "]\n",
    "\n",
    "output = Var_to_nparray(forward(x, NN))\n",
    "\n",
    "x = Var_to_nparray(x)\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faCxhfFnFtHp"
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "I2eDYKvAFtHq"
   },
   "outputs": [],
   "source": [
    "def squared_loss(x, y):\n",
    "    \n",
    "  # add check that sizes agree\n",
    "    def squared_loss_single(t, y):\n",
    "        loss = Var(0.0)\n",
    "        for i in range(len(t)): # sum over outputs\n",
    "            loss += (t[i]-y[i]) ** 2\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    Loss = Var(0.0)\n",
    "    for n in range(len(x)): # sum over training data\n",
    "        Loss += squared_loss_single(x[n],y[n])\n",
    "        \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrwSJ2UWFtHu"
   },
   "source": [
    "## Exercise j) Implement cross entropy loss\n",
    "\n",
    "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
    "$$\n",
    "with $p$ given by the the softmax function in terms of the logits $h$:\n",
    "$$\n",
    "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
    "$$\n",
    "Inserting $p$ in the expression for the loss gives\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
    "$$\n",
    "This is true for $t$ being a one-hot vector. \n",
    "\n",
    "Call the function to convince yourself it works. \n",
    "\n",
    "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
    "\n",
    "Help: You can add these methods in the Var class:\n",
    "\n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "    \n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6nMuxyfzFtHv"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(t, h):\n",
    "     \n",
    "    loss = Var(0.0)\n",
    "    loss_a = 0\n",
    "    # Insert code here\n",
    "    for i in range(len(h)):\n",
    "        SumExp += exp(h[i])\n",
    "        \n",
    "        loss_a += t[i] * h[i]\n",
    "        \n",
    "    LogSumExp = log(SumExp)\n",
    "    \n",
    "    loss = -loss_a + LogSumExp\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fAF5ew4FtHy"
   },
   "source": [
    "# Backward pass\n",
    "\n",
    "Now the magic happens! We get the calculation of the gradients for free. Just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iHyfPPI9Qqwu"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "loss = squared_loss(y_train,output)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49biIAYKQ1oG"
   },
   "source": [
    "and the gradients will be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_rGt1bq_Q7uk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 \n",
      " 1 input nodes \n",
      "5 output nodes \n",
      " Weights:[[Var(v=0.0435, grad=4.1966), Var(v=-0.0705, grad=0.0432), Var(v=-0.1112, grad=9.9956), Var(v=-0.0723, grad=-11.5123), Var(v=-0.1349, grad=5.7343)]] \n",
      " Biases:[Var(v=0.0000, grad=3.6048), Var(v=0.0000, grad=-0.0392), Var(v=0.0000, grad=-9.0836), Var(v=0.0000, grad=10.4620), Var(v=0.0000, grad=-5.2111)]\n",
      "Layer 1 \n",
      " 5 input nodes \n",
      "1 output nodes \n",
      " Weights:[[Var(v=-0.0412, grad=-4.4297)], [Var(v=-0.0005, grad=6.6930)], [Var(v=-0.1053, grad=10.5534)], [Var(v=0.1213, grad=6.8655)], [Var(v=-0.0604, grad=12.7998)]] \n",
      " Biases:[Var(v=0.0000, grad=-5.9799)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7d7qK0uFtH9"
   },
   "source": [
    "# Backward pass unit test\n",
    "\n",
    "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgBi8GOSFtIN"
   },
   "source": [
    "# Training and validation\n",
    "\n",
    "We are ready to train some neural networks!\n",
    "\n",
    "We initialize again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "01ePmzBzRtdh"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 15, lambda x: x.relu()),\n",
    "    DenseLayer(15, 50, lambda x: x.relu()),\n",
    "    DenseLayer(50, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "Loss = squared_loss(y_train,output)\n",
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10iRPiQ1ISHw"
   },
   "source": [
    "and make an update:\n",
    "\n",
    "We introduce a help function parameters to have a handle in all parameters in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dhAI7eyeznia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network before update:\n",
      "Layer 0 \n",
      " 1 input nodes \n",
      "15 output nodes \n",
      " Weights:[[Var(v=-0.0443, grad=0.7083), Var(v=-0.2229, grad=-0.1263), Var(v=0.0026, grad=0.4918), Var(v=-0.0330, grad=-7.9923), Var(v=0.1992, grad=-2.5511), Var(v=-0.0430, grad=-4.1783), Var(v=-0.0993, grad=2.8203), Var(v=-0.1289, grad=1.6250), Var(v=0.0261, grad=3.9231), Var(v=0.1491, grad=-4.8985), Var(v=-0.0197, grad=-2.1117), Var(v=-0.0817, grad=-4.0997), Var(v=-0.0597, grad=7.3440), Var(v=-0.0761, grad=-3.8633), Var(v=-0.1343, grad=-5.1852)]] \n",
      " Biases:[Var(v=0.0000, grad=-0.6439), Var(v=0.0000, grad=0.1149), Var(v=0.0000, grad=0.4226), Var(v=0.0000, grad=7.2658), Var(v=0.0000, grad=-2.1919), Var(v=0.0000, grad=3.7985), Var(v=0.0000, grad=-2.5640), Var(v=0.0000, grad=-1.4773), Var(v=0.0000, grad=3.3707), Var(v=0.0000, grad=-4.2088), Var(v=0.0000, grad=1.9197), Var(v=0.0000, grad=3.7270), Var(v=0.0000, grad=-6.6765), Var(v=0.0000, grad=3.5121), Var(v=0.0000, grad=4.7139)]\n",
      "Layer 1 \n",
      " 15 input nodes \n",
      "50 output nodes \n",
      " Weights:[[Var(v=0.0328, grad=0.0000), Var(v=0.1702, grad=0.2873), Var(v=0.0977, grad=-0.3897), Var(v=-0.0208, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=0.1166, grad=0.3885), Var(v=0.0065, grad=0.9768), Var(v=-0.0109, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=-0.1918, grad=0.0000), Var(v=0.0553, grad=0.0000), Var(v=-0.0593, grad=-0.2709), Var(v=-0.0169, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.1600, grad=-0.2994), Var(v=-0.0356, grad=0.2909), Var(v=-0.0611, grad=0.0000), Var(v=0.1277, grad=0.0000), Var(v=-0.1176, grad=-0.1275), Var(v=0.0902, grad=-0.0613), Var(v=0.0030, grad=0.0000), Var(v=-0.0733, grad=0.2065), Var(v=0.0956, grad=0.0000), Var(v=0.0736, grad=0.0000), Var(v=-0.0269, grad=0.1809), Var(v=0.0931, grad=0.0000), Var(v=-0.0169, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=-0.1294, grad=0.0000), Var(v=-0.2200, grad=0.2429), Var(v=0.0301, grad=-0.1987), Var(v=0.0555, grad=0.0000), Var(v=0.0842, grad=-0.2085), Var(v=-0.0130, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.0853, grad=-0.3434), Var(v=0.0902, grad=0.0452), Var(v=-0.0379, grad=0.4421), Var(v=0.1361, grad=-0.2865), Var(v=-0.0136, grad=0.0000), Var(v=0.0358, grad=0.2362), Var(v=0.1156, grad=0.4339), Var(v=0.0013, grad=-0.6888), Var(v=-0.0062, grad=0.0000), Var(v=0.0265, grad=-0.1419), Var(v=0.0968, grad=0.1468), Var(v=0.1555, grad=-0.0934), Var(v=0.1433, grad=-0.2269), Var(v=0.0514, grad=0.0000)], [Var(v=-0.0939, grad=0.0000), Var(v=-0.0243, grad=1.4466), Var(v=-0.0247, grad=-1.9623), Var(v=-0.0833, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=0.1612, grad=1.9562), Var(v=0.0495, grad=4.9188), Var(v=0.1115, grad=0.0000), Var(v=-0.0913, grad=0.0000), Var(v=0.0538, grad=0.0000), Var(v=-0.0794, grad=0.0000), Var(v=-0.1256, grad=-1.3641), Var(v=0.0087, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.1515, grad=-1.5079), Var(v=0.0156, grad=1.4650), Var(v=-0.1471, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0660, grad=-0.6421), Var(v=0.1051, grad=-0.3088), Var(v=-0.1378, grad=0.0000), Var(v=0.1058, grad=1.0400), Var(v=-0.0980, grad=0.0000), Var(v=-0.1332, grad=0.0000), Var(v=-0.0823, grad=0.9109), Var(v=-0.1635, grad=0.0000), Var(v=0.0499, grad=0.0000), Var(v=0.0616, grad=0.0000), Var(v=-0.0631, grad=0.0000), Var(v=-0.1000, grad=0.0000), Var(v=0.1223, grad=1.2232), Var(v=0.1003, grad=-1.0005), Var(v=-0.0622, grad=0.0000), Var(v=0.0507, grad=-1.0500), Var(v=-0.1276, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=-0.0055, grad=-1.7296), Var(v=-0.0316, grad=0.2278), Var(v=0.0889, grad=2.2264), Var(v=0.1136, grad=-1.4429), Var(v=-0.1227, grad=0.0000), Var(v=-0.1012, grad=1.1895), Var(v=0.0177, grad=2.1850), Var(v=0.0557, grad=-3.4689), Var(v=-0.0981, grad=0.0000), Var(v=0.1608, grad=-0.7148), Var(v=0.0820, grad=0.7395), Var(v=0.1262, grad=-0.4703), Var(v=0.0998, grad=-1.1426), Var(v=-0.0355, grad=0.0000)], [Var(v=0.0997, grad=-0.0279), Var(v=-0.2237, grad=-0.0177), Var(v=0.0060, grad=0.0000), Var(v=-0.2065, grad=-0.0068), Var(v=0.0761, grad=-0.0245), Var(v=0.0374, grad=-0.0240), Var(v=0.0119, grad=-0.0603), Var(v=-0.0676, grad=0.0161), Var(v=-0.0176, grad=-0.0401), Var(v=-0.0231, grad=-0.0250), Var(v=0.0289, grad=0.0000), Var(v=-0.0683, grad=0.0167), Var(v=-0.0982, grad=-0.0112), Var(v=-0.0209, grad=0.0089), Var(v=0.1243, grad=0.0185), Var(v=-0.0512, grad=0.0000), Var(v=-0.0413, grad=0.0000), Var(v=-0.1246, grad=0.0489), Var(v=-0.1129, grad=0.0000), Var(v=0.1229, grad=0.0038), Var(v=-0.0488, grad=0.0000), Var(v=-0.0576, grad=-0.0128), Var(v=0.0085, grad=0.0000), Var(v=0.0321, grad=-0.0051), Var(v=0.0087, grad=0.0000), Var(v=-0.0861, grad=0.0052), Var(v=0.0303, grad=0.0000), Var(v=-0.1115, grad=0.0000), Var(v=0.1346, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.1493, grad=-0.0150), Var(v=-0.0846, grad=0.0000), Var(v=-0.0193, grad=-0.0116), Var(v=-0.1809, grad=0.0129), Var(v=0.0693, grad=0.0315), Var(v=0.0696, grad=0.0141), Var(v=0.0320, grad=0.0000), Var(v=0.0314, grad=-0.0028), Var(v=0.1509, grad=0.0000), Var(v=0.0470, grad=0.0177), Var(v=-0.0798, grad=0.0000), Var(v=-0.0177, grad=0.0000), Var(v=-0.1374, grad=-0.0268), Var(v=0.0292, grad=0.0000), Var(v=-0.1705, grad=-0.0084), Var(v=0.0701, grad=0.0000), Var(v=-0.0395, grad=-0.0091), Var(v=0.0768, grad=0.0058), Var(v=-0.0044, grad=0.0140), Var(v=-0.0344, grad=0.0000)], [Var(v=-0.0201, grad=0.0000), Var(v=0.0597, grad=0.2140), Var(v=0.0595, grad=-0.2903), Var(v=0.0697, grad=0.0000), Var(v=0.1170, grad=0.0000), Var(v=0.0115, grad=0.2894), Var(v=0.0153, grad=0.7276), Var(v=-0.1907, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=-0.0318, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=-0.0548, grad=-0.2018), Var(v=-0.0449, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=0.0485, grad=-0.2231), Var(v=-0.0024, grad=0.2167), Var(v=0.0273, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.0341, grad=-0.0950), Var(v=0.1592, grad=-0.0457), Var(v=-0.1270, grad=0.0000), Var(v=0.0335, grad=0.1538), Var(v=-0.0332, grad=0.0000), Var(v=0.1080, grad=0.0000), Var(v=0.2393, grad=0.1347), Var(v=-0.1662, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0885, grad=0.0000), Var(v=0.0763, grad=0.0000), Var(v=-0.1214, grad=0.0000), Var(v=0.0636, grad=0.1809), Var(v=-0.0640, grad=-0.1480), Var(v=-0.2101, grad=0.0000), Var(v=0.0266, grad=-0.1553), Var(v=-0.0711, grad=0.0000), Var(v=0.0685, grad=0.0000), Var(v=-0.1257, grad=-0.2558), Var(v=-0.0710, grad=0.0337), Var(v=-0.0215, grad=0.3293), Var(v=-0.1976, grad=-0.2134), Var(v=-0.0490, grad=0.0000), Var(v=0.0367, grad=0.1760), Var(v=0.0735, grad=0.3232), Var(v=-0.1036, grad=-0.5131), Var(v=0.0397, grad=0.0000), Var(v=-0.0314, grad=-0.1057), Var(v=0.0926, grad=0.1094), Var(v=0.0574, grad=-0.0696), Var(v=-0.2720, grad=-0.1690), Var(v=-0.0798, grad=0.0000)], [Var(v=0.0158, grad=-2.1056), Var(v=0.0924, grad=-1.3389), Var(v=-0.1050, grad=0.0000), Var(v=-0.0301, grad=-0.5158), Var(v=-0.0237, grad=-1.8481), Var(v=-0.0097, grad=-1.8106), Var(v=0.1733, grad=-4.5525), Var(v=0.1487, grad=1.2177), Var(v=0.0753, grad=-3.0234), Var(v=0.1435, grad=-1.8839), Var(v=-0.0940, grad=0.0000), Var(v=-0.0017, grad=1.2625), Var(v=0.0717, grad=-0.8453), Var(v=0.2430, grad=0.6736), Var(v=0.1638, grad=1.3956), Var(v=-0.0533, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0826, grad=3.6860), Var(v=-0.2483, grad=0.0000), Var(v=0.0104, grad=0.2858), Var(v=-0.0282, grad=0.0000), Var(v=0.0211, grad=-0.9626), Var(v=-0.0176, grad=0.0000), Var(v=0.1195, grad=-0.3829), Var(v=-0.1949, grad=0.0000), Var(v=-0.0522, grad=0.3916), Var(v=-0.0114, grad=0.0000), Var(v=-0.1333, grad=0.0000), Var(v=-0.1367, grad=0.0000), Var(v=0.0176, grad=0.0000), Var(v=0.0358, grad=-1.1322), Var(v=0.0776, grad=0.0000), Var(v=-0.0308, grad=-0.8732), Var(v=0.0542, grad=0.9718), Var(v=0.0604, grad=2.3797), Var(v=0.1895, grad=1.0623), Var(v=-0.0316, grad=0.0000), Var(v=0.1675, grad=-0.2108), Var(v=-0.0994, grad=0.0000), Var(v=0.0539, grad=1.3355), Var(v=-0.0203, grad=0.0000), Var(v=-0.0919, grad=0.0000), Var(v=0.0623, grad=-2.0223), Var(v=0.0322, grad=0.0000), Var(v=0.0171, grad=-0.6326), Var(v=0.0199, grad=0.0000), Var(v=0.1231, grad=-0.6844), Var(v=-0.0131, grad=0.4352), Var(v=-0.0632, grad=1.0575), Var(v=-0.1264, grad=0.0000)], [Var(v=-0.1231, grad=0.0000), Var(v=0.1661, grad=0.2789), Var(v=-0.0259, grad=-0.3784), Var(v=-0.0723, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=0.0928, grad=0.3772), Var(v=0.1630, grad=0.9484), Var(v=0.0788, grad=0.0000), Var(v=-0.0538, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0186, grad=0.0000), Var(v=-0.1493, grad=-0.2630), Var(v=-0.0626, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.1057, grad=-0.2907), Var(v=0.0919, grad=0.2825), Var(v=0.0728, grad=0.0000), Var(v=-0.0458, grad=0.0000), Var(v=-0.0062, grad=-0.1238), Var(v=-0.0104, grad=-0.0595), Var(v=-0.0195, grad=0.0000), Var(v=-0.0748, grad=0.2005), Var(v=-0.2029, grad=0.0000), Var(v=-0.0676, grad=0.0000), Var(v=0.0332, grad=0.1756), Var(v=-0.0543, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0534, grad=0.0000), Var(v=0.0240, grad=0.0000), Var(v=0.1487, grad=0.0000), Var(v=0.1185, grad=0.2358), Var(v=-0.1257, grad=-0.1929), Var(v=0.0560, grad=0.0000), Var(v=-0.1641, grad=-0.2024), Var(v=-0.0374, grad=0.0000), Var(v=0.1036, grad=0.0000), Var(v=0.1169, grad=-0.3335), Var(v=0.0013, grad=0.0439), Var(v=-0.0751, grad=0.4293), Var(v=-0.0179, grad=-0.2782), Var(v=0.0882, grad=0.0000), Var(v=-0.0526, grad=0.2293), Var(v=0.0329, grad=0.4213), Var(v=0.1968, grad=-0.6688), Var(v=-0.0819, grad=0.0000), Var(v=0.0734, grad=-0.1378), Var(v=0.1014, grad=0.1426), Var(v=-0.1477, grad=-0.0907), Var(v=0.0002, grad=-0.2203), Var(v=-0.1711, grad=0.0000)], [Var(v=0.1576, grad=0.0000), Var(v=0.1037, grad=0.6446), Var(v=-0.0309, grad=-0.8745), Var(v=0.0114, grad=0.0000), Var(v=0.0987, grad=0.0000), Var(v=-0.0887, grad=0.8718), Var(v=-0.0012, grad=2.1920), Var(v=-0.1075, grad=0.0000), Var(v=0.0704, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=-0.0807, grad=0.0000), Var(v=0.2467, grad=-0.6079), Var(v=0.0996, grad=0.0000), Var(v=-0.0386, grad=0.0000), Var(v=-0.0601, grad=-0.6720), Var(v=0.1429, grad=0.6529), Var(v=-0.0282, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=0.1092, grad=-0.2862), Var(v=-0.0412, grad=-0.1376), Var(v=-0.0429, grad=0.0000), Var(v=0.0611, grad=0.4635), Var(v=0.1357, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=-0.0540, grad=0.4059), Var(v=0.1266, grad=0.0000), Var(v=-0.1812, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0999, grad=0.0000), Var(v=0.1369, grad=0.0000), Var(v=-0.1354, grad=0.5451), Var(v=0.1759, grad=-0.4459), Var(v=0.0579, grad=0.0000), Var(v=0.1486, grad=-0.4679), Var(v=-0.0895, grad=0.0000), Var(v=0.0481, grad=0.0000), Var(v=0.0925, grad=-0.7707), Var(v=0.0879, grad=0.1015), Var(v=0.0670, grad=0.9922), Var(v=-0.1202, grad=-0.6430), Var(v=0.1125, grad=0.0000), Var(v=0.1173, grad=0.5301), Var(v=-0.0940, grad=0.9737), Var(v=0.0384, grad=-1.5458), Var(v=-0.0428, grad=0.0000), Var(v=-0.0346, grad=-0.3185), Var(v=0.0138, grad=0.3295), Var(v=0.1219, grad=-0.2096), Var(v=0.0380, grad=-0.5092), Var(v=0.0096, grad=0.0000)], [Var(v=-0.0274, grad=0.0000), Var(v=-0.2414, grad=0.8368), Var(v=0.0874, grad=-1.1352), Var(v=-0.0013, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.1090, grad=1.1316), Var(v=0.0055, grad=2.8453), Var(v=-0.0662, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.1152, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.0848, grad=-0.7891), Var(v=-0.1886, grad=0.0000), Var(v=-0.1123, grad=0.0000), Var(v=0.0134, grad=-0.8723), Var(v=-0.0200, grad=0.8475), Var(v=-0.0594, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0599, grad=-0.3714), Var(v=-0.0816, grad=-0.1786), Var(v=0.0785, grad=0.0000), Var(v=-0.1336, grad=0.6016), Var(v=0.0132, grad=0.0000), Var(v=-0.0128, grad=0.0000), Var(v=0.2231, grad=0.5269), Var(v=0.0906, grad=0.0000), Var(v=-0.1521, grad=0.0000), Var(v=-0.2826, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1186, grad=0.0000), Var(v=0.0931, grad=0.7076), Var(v=-0.0134, grad=-0.5788), Var(v=-0.1232, grad=0.0000), Var(v=0.0680, grad=-0.6074), Var(v=-0.0433, grad=0.0000), Var(v=-0.1028, grad=0.0000), Var(v=-0.0755, grad=-1.0005), Var(v=0.0902, grad=0.1318), Var(v=0.0048, grad=1.2879), Var(v=0.0924, grad=-0.8347), Var(v=0.0192, grad=0.0000), Var(v=-0.0971, grad=0.6881), Var(v=0.2161, grad=1.2639), Var(v=0.0097, grad=-2.0066), Var(v=0.0582, grad=0.0000), Var(v=0.0614, grad=-0.4135), Var(v=-0.1421, grad=0.4278), Var(v=0.0453, grad=-0.2720), Var(v=-0.1939, grad=-0.6610), Var(v=-0.1961, grad=0.0000)], [Var(v=-0.0543, grad=-0.2759), Var(v=-0.1041, grad=-0.1754), Var(v=-0.0786, grad=0.0000), Var(v=-0.0813, grad=-0.0676), Var(v=0.0649, grad=-0.2421), Var(v=-0.0837, grad=-0.2372), Var(v=0.0317, grad=-0.5965), Var(v=0.1450, grad=0.1595), Var(v=-0.0966, grad=-0.3961), Var(v=0.0566, grad=-0.2468), Var(v=0.0556, grad=0.0000), Var(v=0.0729, grad=0.1654), Var(v=-0.0199, grad=-0.1108), Var(v=-0.0008, grad=0.0883), Var(v=-0.0910, grad=0.1829), Var(v=0.0664, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0189, grad=0.4829), Var(v=0.1503, grad=0.0000), Var(v=0.1189, grad=0.0374), Var(v=0.0351, grad=0.0000), Var(v=-0.0633, grad=-0.1261), Var(v=0.1342, grad=0.0000), Var(v=-0.0946, grad=-0.0502), Var(v=0.0493, grad=0.0000), Var(v=0.0356, grad=0.0513), Var(v=-0.0448, grad=0.0000), Var(v=-0.0870, grad=0.0000), Var(v=0.0783, grad=0.0000), Var(v=0.0144, grad=0.0000), Var(v=0.0481, grad=-0.1483), Var(v=-0.1935, grad=0.0000), Var(v=0.0104, grad=-0.1144), Var(v=-0.1509, grad=0.1273), Var(v=-0.0390, grad=0.3118), Var(v=0.0193, grad=0.1392), Var(v=0.0132, grad=0.0000), Var(v=-0.2381, grad=-0.0276), Var(v=-0.0518, grad=0.0000), Var(v=0.0530, grad=0.1750), Var(v=-0.1068, grad=0.0000), Var(v=0.0563, grad=0.0000), Var(v=-0.0274, grad=-0.2650), Var(v=0.0133, grad=0.0000), Var(v=0.0862, grad=-0.0829), Var(v=0.0107, grad=0.0000), Var(v=-0.1094, grad=-0.0897), Var(v=0.0132, grad=0.0570), Var(v=0.1141, grad=0.1386), Var(v=-0.0614, grad=0.0000)], [Var(v=0.0540, grad=-1.5760), Var(v=0.1461, grad=-1.0021), Var(v=0.0976, grad=0.0000), Var(v=0.0721, grad=-0.3861), Var(v=0.0847, grad=-1.3833), Var(v=0.0320, grad=-1.3552), Var(v=0.0215, grad=-3.4075), Var(v=-0.0356, grad=0.9114), Var(v=0.0050, grad=-2.2630), Var(v=0.0253, grad=-1.4101), Var(v=0.1086, grad=0.0000), Var(v=0.0650, grad=0.9449), Var(v=0.1540, grad=-0.6327), Var(v=-0.1348, grad=0.5042), Var(v=0.1519, grad=1.0446), Var(v=0.0151, grad=0.0000), Var(v=-0.0092, grad=0.0000), Var(v=-0.0934, grad=2.7589), Var(v=0.0089, grad=0.0000), Var(v=0.2123, grad=0.2139), Var(v=0.0081, grad=0.0000), Var(v=0.2530, grad=-0.7205), Var(v=-0.0009, grad=0.0000), Var(v=0.1112, grad=-0.2866), Var(v=-0.0850, grad=0.0000), Var(v=0.1141, grad=0.2931), Var(v=0.0055, grad=0.0000), Var(v=0.0415, grad=0.0000), Var(v=0.0248, grad=0.0000), Var(v=-0.0288, grad=0.0000), Var(v=-0.0437, grad=-0.8474), Var(v=-0.0815, grad=0.0000), Var(v=0.0465, grad=-0.6536), Var(v=0.2050, grad=0.7274), Var(v=-0.0331, grad=1.7811), Var(v=-0.1176, grad=0.7951), Var(v=-0.0673, grad=0.0000), Var(v=0.0763, grad=-0.1578), Var(v=0.1317, grad=0.0000), Var(v=0.0909, grad=0.9996), Var(v=-0.0143, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=-0.0054, grad=-1.5136), Var(v=-0.1060, grad=0.0000), Var(v=0.0174, grad=-0.4735), Var(v=-0.1463, grad=0.0000), Var(v=-0.0123, grad=-0.5123), Var(v=0.0428, grad=0.3258), Var(v=0.0988, grad=0.7915), Var(v=0.1110, grad=0.0000)], [Var(v=0.2044, grad=0.0000), Var(v=0.1655, grad=0.1277), Var(v=0.0608, grad=-0.1732), Var(v=-0.0533, grad=0.0000), Var(v=-0.0691, grad=0.0000), Var(v=-0.0964, grad=0.1726), Var(v=0.0887, grad=0.4341), Var(v=0.0226, grad=0.0000), Var(v=0.0033, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0659, grad=0.0000), Var(v=0.1335, grad=-0.1204), Var(v=0.0647, grad=0.0000), Var(v=-0.0987, grad=0.0000), Var(v=0.1874, grad=-0.1331), Var(v=0.0397, grad=0.1293), Var(v=-0.0899, grad=0.0000), Var(v=0.0044, grad=0.0000), Var(v=-0.2468, grad=-0.0567), Var(v=-0.1814, grad=-0.0273), Var(v=0.0490, grad=0.0000), Var(v=0.0061, grad=0.0918), Var(v=-0.0139, grad=0.0000), Var(v=0.0396, grad=0.0000), Var(v=0.1300, grad=0.0804), Var(v=-0.1627, grad=0.0000), Var(v=-0.0507, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=0.1070, grad=0.0000), Var(v=-0.1737, grad=0.1080), Var(v=-0.0331, grad=-0.0883), Var(v=-0.0697, grad=0.0000), Var(v=-0.1409, grad=-0.0927), Var(v=0.0276, grad=0.0000), Var(v=0.0108, grad=0.0000), Var(v=-0.2116, grad=-0.1526), Var(v=-0.2144, grad=0.0201), Var(v=-0.0058, grad=0.1965), Var(v=-0.1994, grad=-0.1273), Var(v=0.1132, grad=0.0000), Var(v=0.0561, grad=0.1050), Var(v=-0.0951, grad=0.1928), Var(v=0.0662, grad=-0.3061), Var(v=0.0700, grad=0.0000), Var(v=0.0117, grad=-0.0631), Var(v=0.0949, grad=0.0653), Var(v=-0.0612, grad=-0.0415), Var(v=0.0962, grad=-0.1008), Var(v=-0.0521, grad=0.0000)], [Var(v=-0.1435, grad=0.0000), Var(v=0.0054, grad=0.5305), Var(v=0.1000, grad=-0.7196), Var(v=-0.2017, grad=0.0000), Var(v=0.1438, grad=0.0000), Var(v=0.0086, grad=0.7174), Var(v=-0.0113, grad=1.8038), Var(v=-0.0296, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=0.0540, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0583, grad=-0.5002), Var(v=0.0883, grad=0.0000), Var(v=0.1265, grad=0.0000), Var(v=-0.0333, grad=-0.5530), Var(v=0.0003, grad=0.5372), Var(v=-0.1604, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.1214, grad=-0.2355), Var(v=-0.0132, grad=-0.1132), Var(v=-0.1292, grad=0.0000), Var(v=0.0989, grad=0.3814), Var(v=-0.0757, grad=0.0000), Var(v=0.1149, grad=0.0000), Var(v=0.2081, grad=0.3340), Var(v=-0.2998, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=0.1183, grad=0.4486), Var(v=-0.1514, grad=-0.3669), Var(v=-0.1122, grad=0.0000), Var(v=0.1318, grad=-0.3851), Var(v=0.0861, grad=0.0000), Var(v=-0.0581, grad=0.0000), Var(v=0.0171, grad=-0.6343), Var(v=0.0300, grad=0.0835), Var(v=-0.0124, grad=0.8165), Var(v=-0.0909, grad=-0.5291), Var(v=-0.0582, grad=0.0000), Var(v=0.0930, grad=0.4362), Var(v=0.0423, grad=0.8013), Var(v=-0.0517, grad=-1.2721), Var(v=-0.0017, grad=0.0000), Var(v=-0.0728, grad=-0.2621), Var(v=0.0618, grad=0.2712), Var(v=-0.2105, grad=-0.1725), Var(v=0.0222, grad=-0.4190), Var(v=0.0534, grad=0.0000)], [Var(v=0.0445, grad=0.0000), Var(v=0.0360, grad=0.3877), Var(v=0.0723, grad=-0.5259), Var(v=0.0215, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=-0.0629, grad=0.5243), Var(v=0.0324, grad=1.3183), Var(v=0.1279, grad=0.0000), Var(v=-0.0392, grad=0.0000), Var(v=0.0804, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=-0.0575, grad=-0.3656), Var(v=-0.1589, grad=0.0000), Var(v=-0.0182, grad=0.0000), Var(v=-0.0623, grad=-0.4041), Var(v=0.1969, grad=0.3926), Var(v=0.0346, grad=0.0000), Var(v=0.2010, grad=0.0000), Var(v=-0.0487, grad=-0.1721), Var(v=0.0729, grad=-0.0828), Var(v=0.0323, grad=0.0000), Var(v=-0.1248, grad=0.2787), Var(v=-0.0293, grad=0.0000), Var(v=0.0097, grad=0.0000), Var(v=-0.0138, grad=0.2441), Var(v=0.1279, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0648, grad=0.0000), Var(v=0.0968, grad=0.0000), Var(v=-0.0947, grad=0.3278), Var(v=0.0082, grad=-0.2681), Var(v=-0.0497, grad=0.0000), Var(v=0.0246, grad=-0.2814), Var(v=0.0367, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.1727, grad=-0.4635), Var(v=-0.0029, grad=0.0610), Var(v=-0.0300, grad=0.5967), Var(v=0.0390, grad=-0.3867), Var(v=-0.0694, grad=0.0000), Var(v=0.1484, grad=0.3188), Var(v=-0.1028, grad=0.5856), Var(v=0.2687, grad=-0.9297), Var(v=-0.1080, grad=0.0000), Var(v=0.1325, grad=-0.1916), Var(v=-0.0897, grad=0.1982), Var(v=-0.0730, grad=-0.1260), Var(v=0.2022, grad=-0.3062), Var(v=0.1066, grad=0.0000)], [Var(v=-0.1785, grad=0.0000), Var(v=0.0147, grad=0.4937), Var(v=-0.0011, grad=-0.6697), Var(v=0.0647, grad=0.0000), Var(v=-0.1213, grad=0.0000), Var(v=-0.0001, grad=0.6676), Var(v=0.1840, grad=1.6787), Var(v=-0.1034, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0280, grad=0.0000), Var(v=0.0140, grad=-0.4655), Var(v=0.1532, grad=0.0000), Var(v=-0.0072, grad=0.0000), Var(v=-0.1556, grad=-0.5146), Var(v=-0.0321, grad=0.5000), Var(v=-0.1814, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0768, grad=-0.2192), Var(v=-0.1270, grad=-0.1054), Var(v=-0.0873, grad=0.0000), Var(v=0.0949, grad=0.3550), Var(v=0.1548, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.1330, grad=0.3109), Var(v=-0.1112, grad=0.0000), Var(v=-0.0965, grad=0.0000), Var(v=0.2110, grad=0.0000), Var(v=-0.0255, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=-0.0783, grad=0.4175), Var(v=-0.0120, grad=-0.3415), Var(v=0.0817, grad=0.0000), Var(v=-0.0358, grad=-0.3584), Var(v=-0.0309, grad=0.0000), Var(v=0.1137, grad=0.0000), Var(v=0.0130, grad=-0.5903), Var(v=-0.0736, grad=0.0777), Var(v=-0.1806, grad=0.7599), Var(v=0.1096, grad=-0.4924), Var(v=-0.0586, grad=0.0000), Var(v=0.1502, grad=0.4060), Var(v=-0.1168, grad=0.7457), Var(v=-0.0637, grad=-1.1839), Var(v=0.0139, grad=0.0000), Var(v=-0.2013, grad=-0.2439), Var(v=0.0755, grad=0.2524), Var(v=-0.1206, grad=-0.1605), Var(v=-0.0362, grad=-0.3900), Var(v=-0.0711, grad=0.0000)], [Var(v=0.0680, grad=0.0000), Var(v=0.0223, grad=0.8719), Var(v=0.0893, grad=-1.1827), Var(v=-0.1347, grad=0.0000), Var(v=-0.1765, grad=0.0000), Var(v=0.0676, grad=1.1790), Var(v=0.2246, grad=2.9646), Var(v=-0.0265, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=-0.2653, grad=0.0000), Var(v=0.1237, grad=-0.8221), Var(v=0.0243, grad=0.0000), Var(v=-0.0558, grad=0.0000), Var(v=-0.0360, grad=-0.9088), Var(v=-0.0556, grad=0.8830), Var(v=-0.0403, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0888, grad=-0.3870), Var(v=-0.0184, grad=-0.1861), Var(v=-0.1433, grad=0.0000), Var(v=-0.0176, grad=0.6268), Var(v=-0.1179, grad=0.0000), Var(v=0.0957, grad=0.0000), Var(v=-0.0842, grad=0.5490), Var(v=-0.0365, grad=0.0000), Var(v=0.0466, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0515, grad=0.0000), Var(v=-0.0993, grad=0.0000), Var(v=0.0273, grad=0.7373), Var(v=0.0668, grad=-0.6030), Var(v=0.1252, grad=0.0000), Var(v=0.0060, grad=-0.6329), Var(v=0.0019, grad=0.0000), Var(v=0.0220, grad=0.0000), Var(v=0.0097, grad=-1.0424), Var(v=0.0546, grad=0.1373), Var(v=0.0049, grad=1.3419), Var(v=0.0273, grad=-0.8696), Var(v=-0.0136, grad=0.0000), Var(v=0.0559, grad=0.7169), Var(v=-0.0272, grad=1.3169), Var(v=-0.0986, grad=-2.0907), Var(v=-0.0018, grad=0.0000), Var(v=-0.1115, grad=-0.4308), Var(v=0.0492, grad=0.4457), Var(v=0.0922, grad=-0.2834), Var(v=0.0518, grad=-0.6887), Var(v=-0.0644, grad=0.0000)]] \n",
      " Biases:[Var(v=0.0000, grad=-136.2061), Var(v=0.0000, grad=1.8956), Var(v=0.0000, grad=-120.0617), Var(v=0.0000, grad=-33.3666), Var(v=0.0000, grad=-119.5546), Var(v=0.0000, grad=2.5634), Var(v=0.0000, grad=6.4455), Var(v=0.0000, grad=78.7737), Var(v=0.0000, grad=-195.5812), Var(v=0.0000, grad=-121.8685), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-1.7874), Var(v=0.0000, grad=-54.6822), Var(v=0.0000, grad=43.5727), Var(v=0.0000, grad=-1.9759), Var(v=0.0000, grad=89.6326), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=238.4436), Var(v=0.0000, grad=-39.2869), Var(v=0.0000, grad=-0.4046), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=1.3629), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=-24.7699), Var(v=0.0000, grad=55.7286), Var(v=0.0000, grad=25.3302), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=1.6029), Var(v=0.0000, grad=-61.2135), Var(v=0.0000, grad=-56.4850), Var(v=0.0000, grad=-1.3759), Var(v=0.0000, grad=153.9375), Var(v=0.0000, grad=68.7177), Var(v=0.0000, grad=-105.8185), Var(v=0.0000, grad=0.2985), Var(v=0.0000, grad=136.2195), Var(v=0.0000, grad=-1.8908), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=72.7763), Var(v=0.0000, grad=2.8632), Var(v=0.0000, grad=-212.2335), Var(v=0.0000, grad=-40.9236), Var(v=0.0000, grad=-43.7325), Var(v=0.0000, grad=0.9690), Var(v=0.0000, grad=-0.6162), Var(v=0.0000, grad=-1.4973), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " 50 input nodes \n",
      "1 output nodes \n",
      " Weights:[[Var(v=0.1053, grad=-1.0069)], [Var(v=0.0670, grad=-3.6671)], [Var(v=-0.0908, grad=3.2431)], [Var(v=0.0258, grad=-0.2103)], [Var(v=0.0924, grad=-0.9837)], [Var(v=0.0906, grad=2.5128)], [Var(v=0.2277, grad=2.5585)], [Var(v=-0.0609, grad=-2.8014)], [Var(v=0.1512, grad=-1.3236)], [Var(v=0.0942, grad=-3.3882)], [Var(v=0.1012, grad=0.0000)], [Var(v=-0.0631, grad=-0.2671)], [Var(v=0.0423, grad=-3.6596)], [Var(v=-0.0337, grad=-2.8335)], [Var(v=-0.0698, grad=-4.4556)], [Var(v=0.0678, grad=1.9429)], [Var(v=-0.1217, grad=0.0000)], [Var(v=-0.1844, grad=-0.2703)], [Var(v=-0.0297, grad=2.2048)], [Var(v=-0.0143, grad=-3.2260)], [Var(v=-0.0872, grad=0.0000)], [Var(v=0.0481, grad=-2.7974)], [Var(v=0.1191, grad=0.0000)], [Var(v=0.0192, grad=-3.8135)], [Var(v=0.0422, grad=1.0188)], [Var(v=-0.0196, grad=-0.7347)], [Var(v=-0.0579, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0474, grad=0.0000)], [Var(v=0.0069, grad=0.0000)], [Var(v=0.0566, grad=1.8619)], [Var(v=-0.0463, grad=2.6609)], [Var(v=0.0437, grad=-0.1016)], [Var(v=-0.0486, grad=0.1644)], [Var(v=-0.1190, grad=-0.6284)], [Var(v=-0.0531, grad=-2.0964)], [Var(v=-0.0801, grad=1.2349)], [Var(v=0.0105, grad=-2.4381)], [Var(v=0.1031, grad=0.5319)], [Var(v=-0.0668, grad=0.0313)], [Var(v=0.1293, grad=0.0000)], [Var(v=0.0551, grad=1.3201)], [Var(v=0.1012, grad=0.3312)], [Var(v=-0.1606, grad=1.7070)], [Var(v=0.0316, grad=-0.7833)], [Var(v=-0.0331, grad=1.5052)], [Var(v=0.0342, grad=0.6295)], [Var(v=-0.0218, grad=2.3706)], [Var(v=-0.0529, grad=1.2691)], [Var(v=0.2566, grad=0.0000)]] \n",
      " Biases:[Var(v=0.0000, grad=94.3531)]\n",
      "\n",
      "Network after update:\n",
      "Layer 0 \n",
      " 1 input nodes \n",
      "15 output nodes \n",
      " Weights:[[Var(v=-0.0513, grad=0.7083), Var(v=-0.2216, grad=-0.1263), Var(v=-0.0023, grad=0.4918), Var(v=0.0470, grad=-7.9923), Var(v=0.2247, grad=-2.5511), Var(v=-0.0012, grad=-4.1783), Var(v=-0.1275, grad=2.8203), Var(v=-0.1452, grad=1.6250), Var(v=-0.0131, grad=3.9231), Var(v=0.1981, grad=-4.8985), Var(v=0.0014, grad=-2.1117), Var(v=-0.0407, grad=-4.0997), Var(v=-0.1332, grad=7.3440), Var(v=-0.0374, grad=-3.8633), Var(v=-0.0825, grad=-5.1852)]] \n",
      " Biases:[Var(v=0.0064, grad=-0.6439), Var(v=-0.0011, grad=0.1149), Var(v=-0.0042, grad=0.4226), Var(v=-0.0727, grad=7.2658), Var(v=0.0219, grad=-2.1919), Var(v=-0.0380, grad=3.7985), Var(v=0.0256, grad=-2.5640), Var(v=0.0148, grad=-1.4773), Var(v=-0.0337, grad=3.3707), Var(v=0.0421, grad=-4.2088), Var(v=-0.0192, grad=1.9197), Var(v=-0.0373, grad=3.7270), Var(v=0.0668, grad=-6.6765), Var(v=-0.0351, grad=3.5121), Var(v=-0.0471, grad=4.7139)]\n",
      "Layer 1 \n",
      " 15 input nodes \n",
      "50 output nodes \n",
      " Weights:[[Var(v=0.0328, grad=0.0000), Var(v=0.1673, grad=0.2873), Var(v=0.1016, grad=-0.3897), Var(v=-0.0208, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=0.1127, grad=0.3885), Var(v=-0.0033, grad=0.9768), Var(v=-0.0109, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=-0.1918, grad=0.0000), Var(v=0.0553, grad=0.0000), Var(v=-0.0566, grad=-0.2709), Var(v=-0.0169, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.1570, grad=-0.2994), Var(v=-0.0385, grad=0.2909), Var(v=-0.0611, grad=0.0000), Var(v=0.1277, grad=0.0000), Var(v=-0.1164, grad=-0.1275), Var(v=0.0908, grad=-0.0613), Var(v=0.0030, grad=0.0000), Var(v=-0.0754, grad=0.2065), Var(v=0.0956, grad=0.0000), Var(v=0.0736, grad=0.0000), Var(v=-0.0288, grad=0.1809), Var(v=0.0931, grad=0.0000), Var(v=-0.0169, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=-0.1294, grad=0.0000), Var(v=-0.2225, grad=0.2429), Var(v=0.0321, grad=-0.1987), Var(v=0.0555, grad=0.0000), Var(v=0.0863, grad=-0.2085), Var(v=-0.0130, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.0887, grad=-0.3434), Var(v=0.0898, grad=0.0452), Var(v=-0.0423, grad=0.4421), Var(v=0.1390, grad=-0.2865), Var(v=-0.0136, grad=0.0000), Var(v=0.0335, grad=0.2362), Var(v=0.1113, grad=0.4339), Var(v=0.0082, grad=-0.6888), Var(v=-0.0062, grad=0.0000), Var(v=0.0279, grad=-0.1419), Var(v=0.0953, grad=0.1468), Var(v=0.1564, grad=-0.0934), Var(v=0.1456, grad=-0.2269), Var(v=0.0514, grad=0.0000)], [Var(v=-0.0939, grad=0.0000), Var(v=-0.0387, grad=1.4466), Var(v=-0.0051, grad=-1.9623), Var(v=-0.0833, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=0.1416, grad=1.9562), Var(v=0.0003, grad=4.9188), Var(v=0.1115, grad=0.0000), Var(v=-0.0913, grad=0.0000), Var(v=0.0538, grad=0.0000), Var(v=-0.0794, grad=0.0000), Var(v=-0.1119, grad=-1.3641), Var(v=0.0087, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.1666, grad=-1.5079), Var(v=0.0010, grad=1.4650), Var(v=-0.1471, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0724, grad=-0.6421), Var(v=0.1082, grad=-0.3088), Var(v=-0.1378, grad=0.0000), Var(v=0.0954, grad=1.0400), Var(v=-0.0980, grad=0.0000), Var(v=-0.1332, grad=0.0000), Var(v=-0.0914, grad=0.9109), Var(v=-0.1635, grad=0.0000), Var(v=0.0499, grad=0.0000), Var(v=0.0616, grad=0.0000), Var(v=-0.0631, grad=0.0000), Var(v=-0.1000, grad=0.0000), Var(v=0.1100, grad=1.2232), Var(v=0.1103, grad=-1.0005), Var(v=-0.0622, grad=0.0000), Var(v=0.0612, grad=-1.0500), Var(v=-0.1276, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=0.0118, grad=-1.7296), Var(v=-0.0339, grad=0.2278), Var(v=0.0667, grad=2.2264), Var(v=0.1280, grad=-1.4429), Var(v=-0.1227, grad=0.0000), Var(v=-0.1131, grad=1.1895), Var(v=-0.0041, grad=2.1850), Var(v=0.0904, grad=-3.4689), Var(v=-0.0981, grad=0.0000), Var(v=0.1680, grad=-0.7148), Var(v=0.0746, grad=0.7395), Var(v=0.1309, grad=-0.4703), Var(v=0.1112, grad=-1.1426), Var(v=-0.0355, grad=0.0000)], [Var(v=0.1000, grad=-0.0279), Var(v=-0.2235, grad=-0.0177), Var(v=0.0060, grad=0.0000), Var(v=-0.2065, grad=-0.0068), Var(v=0.0763, grad=-0.0245), Var(v=0.0376, grad=-0.0240), Var(v=0.0125, grad=-0.0603), Var(v=-0.0677, grad=0.0161), Var(v=-0.0172, grad=-0.0401), Var(v=-0.0229, grad=-0.0250), Var(v=0.0289, grad=0.0000), Var(v=-0.0684, grad=0.0167), Var(v=-0.0981, grad=-0.0112), Var(v=-0.0210, grad=0.0089), Var(v=0.1241, grad=0.0185), Var(v=-0.0512, grad=0.0000), Var(v=-0.0413, grad=0.0000), Var(v=-0.1251, grad=0.0489), Var(v=-0.1129, grad=0.0000), Var(v=0.1228, grad=0.0038), Var(v=-0.0488, grad=0.0000), Var(v=-0.0575, grad=-0.0128), Var(v=0.0085, grad=0.0000), Var(v=0.0321, grad=-0.0051), Var(v=0.0087, grad=0.0000), Var(v=-0.0862, grad=0.0052), Var(v=0.0303, grad=0.0000), Var(v=-0.1115, grad=0.0000), Var(v=0.1346, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.1494, grad=-0.0150), Var(v=-0.0846, grad=0.0000), Var(v=-0.0192, grad=-0.0116), Var(v=-0.1810, grad=0.0129), Var(v=0.0690, grad=0.0315), Var(v=0.0694, grad=0.0141), Var(v=0.0320, grad=0.0000), Var(v=0.0314, grad=-0.0028), Var(v=0.1509, grad=0.0000), Var(v=0.0468, grad=0.0177), Var(v=-0.0798, grad=0.0000), Var(v=-0.0177, grad=0.0000), Var(v=-0.1371, grad=-0.0268), Var(v=0.0292, grad=0.0000), Var(v=-0.1704, grad=-0.0084), Var(v=0.0701, grad=0.0000), Var(v=-0.0394, grad=-0.0091), Var(v=0.0768, grad=0.0058), Var(v=-0.0045, grad=0.0140), Var(v=-0.0344, grad=0.0000)], [Var(v=-0.0201, grad=0.0000), Var(v=0.0575, grad=0.2140), Var(v=0.0624, grad=-0.2903), Var(v=0.0697, grad=0.0000), Var(v=0.1170, grad=0.0000), Var(v=0.0086, grad=0.2894), Var(v=0.0080, grad=0.7276), Var(v=-0.1907, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=-0.0318, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=-0.0527, grad=-0.2018), Var(v=-0.0449, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=0.0507, grad=-0.2231), Var(v=-0.0046, grad=0.2167), Var(v=0.0273, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.0331, grad=-0.0950), Var(v=0.1597, grad=-0.0457), Var(v=-0.1270, grad=0.0000), Var(v=0.0320, grad=0.1538), Var(v=-0.0332, grad=0.0000), Var(v=0.1080, grad=0.0000), Var(v=0.2380, grad=0.1347), Var(v=-0.1662, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0885, grad=0.0000), Var(v=0.0763, grad=0.0000), Var(v=-0.1214, grad=0.0000), Var(v=0.0618, grad=0.1809), Var(v=-0.0625, grad=-0.1480), Var(v=-0.2101, grad=0.0000), Var(v=0.0282, grad=-0.1553), Var(v=-0.0711, grad=0.0000), Var(v=0.0685, grad=0.0000), Var(v=-0.1231, grad=-0.2558), Var(v=-0.0713, grad=0.0337), Var(v=-0.0248, grad=0.3293), Var(v=-0.1955, grad=-0.2134), Var(v=-0.0490, grad=0.0000), Var(v=0.0349, grad=0.1760), Var(v=0.0703, grad=0.3232), Var(v=-0.0985, grad=-0.5131), Var(v=0.0397, grad=0.0000), Var(v=-0.0304, grad=-0.1057), Var(v=0.0915, grad=0.1094), Var(v=0.0581, grad=-0.0696), Var(v=-0.2703, grad=-0.1690), Var(v=-0.0798, grad=0.0000)], [Var(v=0.0368, grad=-2.1056), Var(v=0.1058, grad=-1.3389), Var(v=-0.1050, grad=0.0000), Var(v=-0.0249, grad=-0.5158), Var(v=-0.0052, grad=-1.8481), Var(v=0.0084, grad=-1.8106), Var(v=0.2188, grad=-4.5525), Var(v=0.1365, grad=1.2177), Var(v=0.1056, grad=-3.0234), Var(v=0.1623, grad=-1.8839), Var(v=-0.0940, grad=0.0000), Var(v=-0.0143, grad=1.2625), Var(v=0.0801, grad=-0.8453), Var(v=0.2363, grad=0.6736), Var(v=0.1498, grad=1.3956), Var(v=-0.0533, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0458, grad=3.6860), Var(v=-0.2483, grad=0.0000), Var(v=0.0076, grad=0.2858), Var(v=-0.0282, grad=0.0000), Var(v=0.0307, grad=-0.9626), Var(v=-0.0176, grad=0.0000), Var(v=0.1233, grad=-0.3829), Var(v=-0.1949, grad=0.0000), Var(v=-0.0561, grad=0.3916), Var(v=-0.0114, grad=0.0000), Var(v=-0.1333, grad=0.0000), Var(v=-0.1367, grad=0.0000), Var(v=0.0176, grad=0.0000), Var(v=0.0471, grad=-1.1322), Var(v=0.0776, grad=0.0000), Var(v=-0.0221, grad=-0.8732), Var(v=0.0444, grad=0.9718), Var(v=0.0366, grad=2.3797), Var(v=0.1788, grad=1.0623), Var(v=-0.0316, grad=0.0000), Var(v=0.1696, grad=-0.2108), Var(v=-0.0994, grad=0.0000), Var(v=0.0405, grad=1.3355), Var(v=-0.0203, grad=0.0000), Var(v=-0.0919, grad=0.0000), Var(v=0.0826, grad=-2.0223), Var(v=0.0322, grad=0.0000), Var(v=0.0234, grad=-0.6326), Var(v=0.0199, grad=0.0000), Var(v=0.1300, grad=-0.6844), Var(v=-0.0174, grad=0.4352), Var(v=-0.0738, grad=1.0575), Var(v=-0.1264, grad=0.0000)], [Var(v=-0.1231, grad=0.0000), Var(v=0.1633, grad=0.2789), Var(v=-0.0221, grad=-0.3784), Var(v=-0.0723, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=0.0890, grad=0.3772), Var(v=0.1536, grad=0.9484), Var(v=0.0788, grad=0.0000), Var(v=-0.0538, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0186, grad=0.0000), Var(v=-0.1466, grad=-0.2630), Var(v=-0.0626, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.1086, grad=-0.2907), Var(v=0.0890, grad=0.2825), Var(v=0.0728, grad=0.0000), Var(v=-0.0458, grad=0.0000), Var(v=-0.0049, grad=-0.1238), Var(v=-0.0098, grad=-0.0595), Var(v=-0.0195, grad=0.0000), Var(v=-0.0768, grad=0.2005), Var(v=-0.2029, grad=0.0000), Var(v=-0.0676, grad=0.0000), Var(v=0.0315, grad=0.1756), Var(v=-0.0543, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0534, grad=0.0000), Var(v=0.0240, grad=0.0000), Var(v=0.1487, grad=0.0000), Var(v=0.1162, grad=0.2358), Var(v=-0.1238, grad=-0.1929), Var(v=0.0560, grad=0.0000), Var(v=-0.1621, grad=-0.2024), Var(v=-0.0374, grad=0.0000), Var(v=0.1036, grad=0.0000), Var(v=0.1203, grad=-0.3335), Var(v=0.0009, grad=0.0439), Var(v=-0.0794, grad=0.4293), Var(v=-0.0151, grad=-0.2782), Var(v=0.0882, grad=0.0000), Var(v=-0.0549, grad=0.2293), Var(v=0.0287, grad=0.4213), Var(v=0.2035, grad=-0.6688), Var(v=-0.0819, grad=0.0000), Var(v=0.0748, grad=-0.1378), Var(v=0.1000, grad=0.1426), Var(v=-0.1468, grad=-0.0907), Var(v=0.0025, grad=-0.2203), Var(v=-0.1711, grad=0.0000)], [Var(v=0.1576, grad=0.0000), Var(v=0.0972, grad=0.6446), Var(v=-0.0222, grad=-0.8745), Var(v=0.0114, grad=0.0000), Var(v=0.0987, grad=0.0000), Var(v=-0.0974, grad=0.8718), Var(v=-0.0231, grad=2.1920), Var(v=-0.1075, grad=0.0000), Var(v=0.0704, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=-0.0807, grad=0.0000), Var(v=0.2528, grad=-0.6079), Var(v=0.0996, grad=0.0000), Var(v=-0.0386, grad=0.0000), Var(v=-0.0534, grad=-0.6720), Var(v=0.1363, grad=0.6529), Var(v=-0.0282, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=0.1121, grad=-0.2862), Var(v=-0.0398, grad=-0.1376), Var(v=-0.0429, grad=0.0000), Var(v=0.0565, grad=0.4635), Var(v=0.1357, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=-0.0580, grad=0.4059), Var(v=0.1266, grad=0.0000), Var(v=-0.1812, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0999, grad=0.0000), Var(v=0.1369, grad=0.0000), Var(v=-0.1409, grad=0.5451), Var(v=0.1803, grad=-0.4459), Var(v=0.0579, grad=0.0000), Var(v=0.1533, grad=-0.4679), Var(v=-0.0895, grad=0.0000), Var(v=0.0481, grad=0.0000), Var(v=0.1003, grad=-0.7707), Var(v=0.0869, grad=0.1015), Var(v=0.0571, grad=0.9922), Var(v=-0.1137, grad=-0.6430), Var(v=0.1125, grad=0.0000), Var(v=0.1120, grad=0.5301), Var(v=-0.1037, grad=0.9737), Var(v=0.0539, grad=-1.5458), Var(v=-0.0428, grad=0.0000), Var(v=-0.0314, grad=-0.3185), Var(v=0.0105, grad=0.3295), Var(v=0.1240, grad=-0.2096), Var(v=0.0431, grad=-0.5092), Var(v=0.0096, grad=0.0000)], [Var(v=-0.0274, grad=0.0000), Var(v=-0.2498, grad=0.8368), Var(v=0.0987, grad=-1.1352), Var(v=-0.0013, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.1204, grad=1.1316), Var(v=-0.0229, grad=2.8453), Var(v=-0.0662, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.1152, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.0927, grad=-0.7891), Var(v=-0.1886, grad=0.0000), Var(v=-0.1123, grad=0.0000), Var(v=0.0221, grad=-0.8723), Var(v=-0.0284, grad=0.8475), Var(v=-0.0594, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0637, grad=-0.3714), Var(v=-0.0798, grad=-0.1786), Var(v=0.0785, grad=0.0000), Var(v=-0.1396, grad=0.6016), Var(v=0.0132, grad=0.0000), Var(v=-0.0128, grad=0.0000), Var(v=0.2178, grad=0.5269), Var(v=0.0906, grad=0.0000), Var(v=-0.1521, grad=0.0000), Var(v=-0.2826, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1186, grad=0.0000), Var(v=0.0861, grad=0.7076), Var(v=-0.0076, grad=-0.5788), Var(v=-0.1232, grad=0.0000), Var(v=0.0740, grad=-0.6074), Var(v=-0.0433, grad=0.0000), Var(v=-0.1028, grad=0.0000), Var(v=-0.0655, grad=-1.0005), Var(v=0.0889, grad=0.1318), Var(v=-0.0081, grad=1.2879), Var(v=0.1007, grad=-0.8347), Var(v=0.0192, grad=0.0000), Var(v=-0.1040, grad=0.6881), Var(v=0.2035, grad=1.2639), Var(v=0.0298, grad=-2.0066), Var(v=0.0582, grad=0.0000), Var(v=0.0656, grad=-0.4135), Var(v=-0.1463, grad=0.4278), Var(v=0.0481, grad=-0.2720), Var(v=-0.1873, grad=-0.6610), Var(v=-0.1961, grad=0.0000)], [Var(v=-0.0516, grad=-0.2759), Var(v=-0.1024, grad=-0.1754), Var(v=-0.0786, grad=0.0000), Var(v=-0.0806, grad=-0.0676), Var(v=0.0673, grad=-0.2421), Var(v=-0.0813, grad=-0.2372), Var(v=0.0377, grad=-0.5965), Var(v=0.1434, grad=0.1595), Var(v=-0.0927, grad=-0.3961), Var(v=0.0590, grad=-0.2468), Var(v=0.0556, grad=0.0000), Var(v=0.0712, grad=0.1654), Var(v=-0.0188, grad=-0.1108), Var(v=-0.0017, grad=0.0883), Var(v=-0.0929, grad=0.1829), Var(v=0.0664, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0141, grad=0.4829), Var(v=0.1503, grad=0.0000), Var(v=0.1185, grad=0.0374), Var(v=0.0351, grad=0.0000), Var(v=-0.0620, grad=-0.1261), Var(v=0.1342, grad=0.0000), Var(v=-0.0941, grad=-0.0502), Var(v=0.0493, grad=0.0000), Var(v=0.0351, grad=0.0513), Var(v=-0.0448, grad=0.0000), Var(v=-0.0870, grad=0.0000), Var(v=0.0783, grad=0.0000), Var(v=0.0144, grad=0.0000), Var(v=0.0496, grad=-0.1483), Var(v=-0.1935, grad=0.0000), Var(v=0.0116, grad=-0.1144), Var(v=-0.1522, grad=0.1273), Var(v=-0.0421, grad=0.3118), Var(v=0.0179, grad=0.1392), Var(v=0.0132, grad=0.0000), Var(v=-0.2378, grad=-0.0276), Var(v=-0.0518, grad=0.0000), Var(v=0.0513, grad=0.1750), Var(v=-0.1068, grad=0.0000), Var(v=0.0563, grad=0.0000), Var(v=-0.0248, grad=-0.2650), Var(v=0.0133, grad=0.0000), Var(v=0.0870, grad=-0.0829), Var(v=0.0107, grad=0.0000), Var(v=-0.1085, grad=-0.0897), Var(v=0.0126, grad=0.0570), Var(v=0.1127, grad=0.1386), Var(v=-0.0614, grad=0.0000)], [Var(v=0.0697, grad=-1.5760), Var(v=0.1562, grad=-1.0021), Var(v=0.0976, grad=0.0000), Var(v=0.0760, grad=-0.3861), Var(v=0.0986, grad=-1.3833), Var(v=0.0456, grad=-1.3552), Var(v=0.0555, grad=-3.4075), Var(v=-0.0447, grad=0.9114), Var(v=0.0277, grad=-2.2630), Var(v=0.0394, grad=-1.4101), Var(v=0.1086, grad=0.0000), Var(v=0.0555, grad=0.9449), Var(v=0.1604, grad=-0.6327), Var(v=-0.1398, grad=0.5042), Var(v=0.1415, grad=1.0446), Var(v=0.0151, grad=0.0000), Var(v=-0.0092, grad=0.0000), Var(v=-0.1210, grad=2.7589), Var(v=0.0089, grad=0.0000), Var(v=0.2101, grad=0.2139), Var(v=0.0081, grad=0.0000), Var(v=0.2602, grad=-0.7205), Var(v=-0.0009, grad=0.0000), Var(v=0.1141, grad=-0.2866), Var(v=-0.0850, grad=0.0000), Var(v=0.1112, grad=0.2931), Var(v=0.0055, grad=0.0000), Var(v=0.0415, grad=0.0000), Var(v=0.0248, grad=0.0000), Var(v=-0.0288, grad=0.0000), Var(v=-0.0353, grad=-0.8474), Var(v=-0.0815, grad=0.0000), Var(v=0.0530, grad=-0.6536), Var(v=0.1977, grad=0.7274), Var(v=-0.0509, grad=1.7811), Var(v=-0.1256, grad=0.7951), Var(v=-0.0673, grad=0.0000), Var(v=0.0779, grad=-0.1578), Var(v=0.1317, grad=0.0000), Var(v=0.0809, grad=0.9996), Var(v=-0.0143, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=0.0098, grad=-1.5136), Var(v=-0.1060, grad=0.0000), Var(v=0.0222, grad=-0.4735), Var(v=-0.1463, grad=0.0000), Var(v=-0.0071, grad=-0.5123), Var(v=0.0396, grad=0.3258), Var(v=0.0909, grad=0.7915), Var(v=0.1110, grad=0.0000)], [Var(v=0.2044, grad=0.0000), Var(v=0.1642, grad=0.1277), Var(v=0.0626, grad=-0.1732), Var(v=-0.0533, grad=0.0000), Var(v=-0.0691, grad=0.0000), Var(v=-0.0981, grad=0.1726), Var(v=0.0843, grad=0.4341), Var(v=0.0226, grad=0.0000), Var(v=0.0033, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0659, grad=0.0000), Var(v=0.1348, grad=-0.1204), Var(v=0.0647, grad=0.0000), Var(v=-0.0987, grad=0.0000), Var(v=0.1888, grad=-0.1331), Var(v=0.0384, grad=0.1293), Var(v=-0.0899, grad=0.0000), Var(v=0.0044, grad=0.0000), Var(v=-0.2462, grad=-0.0567), Var(v=-0.1811, grad=-0.0273), Var(v=0.0490, grad=0.0000), Var(v=0.0052, grad=0.0918), Var(v=-0.0139, grad=0.0000), Var(v=0.0396, grad=0.0000), Var(v=0.1291, grad=0.0804), Var(v=-0.1627, grad=0.0000), Var(v=-0.0507, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=0.1070, grad=0.0000), Var(v=-0.1748, grad=0.1080), Var(v=-0.0323, grad=-0.0883), Var(v=-0.0697, grad=0.0000), Var(v=-0.1400, grad=-0.0927), Var(v=0.0276, grad=0.0000), Var(v=0.0108, grad=0.0000), Var(v=-0.2101, grad=-0.1526), Var(v=-0.2146, grad=0.0201), Var(v=-0.0078, grad=0.1965), Var(v=-0.1981, grad=-0.1273), Var(v=0.1132, grad=0.0000), Var(v=0.0550, grad=0.1050), Var(v=-0.0971, grad=0.1928), Var(v=0.0693, grad=-0.3061), Var(v=0.0700, grad=0.0000), Var(v=0.0124, grad=-0.0631), Var(v=0.0942, grad=0.0653), Var(v=-0.0608, grad=-0.0415), Var(v=0.0973, grad=-0.1008), Var(v=-0.0521, grad=0.0000)], [Var(v=-0.1435, grad=0.0000), Var(v=0.0001, grad=0.5305), Var(v=0.1072, grad=-0.7196), Var(v=-0.2017, grad=0.0000), Var(v=0.1438, grad=0.0000), Var(v=0.0014, grad=0.7174), Var(v=-0.0293, grad=1.8038), Var(v=-0.0296, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=0.0540, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0533, grad=-0.5002), Var(v=0.0883, grad=0.0000), Var(v=0.1265, grad=0.0000), Var(v=-0.0277, grad=-0.5530), Var(v=-0.0051, grad=0.5372), Var(v=-0.1604, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.1238, grad=-0.2355), Var(v=-0.0121, grad=-0.1132), Var(v=-0.1292, grad=0.0000), Var(v=0.0951, grad=0.3814), Var(v=-0.0757, grad=0.0000), Var(v=0.1149, grad=0.0000), Var(v=0.2048, grad=0.3340), Var(v=-0.2998, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=0.1138, grad=0.4486), Var(v=-0.1478, grad=-0.3669), Var(v=-0.1122, grad=0.0000), Var(v=0.1357, grad=-0.3851), Var(v=0.0861, grad=0.0000), Var(v=-0.0581, grad=0.0000), Var(v=0.0235, grad=-0.6343), Var(v=0.0292, grad=0.0835), Var(v=-0.0205, grad=0.8165), Var(v=-0.0856, grad=-0.5291), Var(v=-0.0582, grad=0.0000), Var(v=0.0886, grad=0.4362), Var(v=0.0343, grad=0.8013), Var(v=-0.0389, grad=-1.2721), Var(v=-0.0017, grad=0.0000), Var(v=-0.0701, grad=-0.2621), Var(v=0.0591, grad=0.2712), Var(v=-0.2088, grad=-0.1725), Var(v=0.0264, grad=-0.4190), Var(v=0.0534, grad=0.0000)], [Var(v=0.0445, grad=0.0000), Var(v=0.0321, grad=0.3877), Var(v=0.0776, grad=-0.5259), Var(v=0.0215, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=-0.0682, grad=0.5243), Var(v=0.0192, grad=1.3183), Var(v=0.1279, grad=0.0000), Var(v=-0.0392, grad=0.0000), Var(v=0.0804, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=-0.0538, grad=-0.3656), Var(v=-0.1589, grad=0.0000), Var(v=-0.0182, grad=0.0000), Var(v=-0.0583, grad=-0.4041), Var(v=0.1930, grad=0.3926), Var(v=0.0346, grad=0.0000), Var(v=0.2010, grad=0.0000), Var(v=-0.0470, grad=-0.1721), Var(v=0.0737, grad=-0.0828), Var(v=0.0323, grad=0.0000), Var(v=-0.1276, grad=0.2787), Var(v=-0.0293, grad=0.0000), Var(v=0.0097, grad=0.0000), Var(v=-0.0162, grad=0.2441), Var(v=0.1279, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0648, grad=0.0000), Var(v=0.0968, grad=0.0000), Var(v=-0.0980, grad=0.3278), Var(v=0.0109, grad=-0.2681), Var(v=-0.0497, grad=0.0000), Var(v=0.0274, grad=-0.2814), Var(v=0.0367, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.1774, grad=-0.4635), Var(v=-0.0035, grad=0.0610), Var(v=-0.0359, grad=0.5967), Var(v=0.0429, grad=-0.3867), Var(v=-0.0694, grad=0.0000), Var(v=0.1452, grad=0.3188), Var(v=-0.1087, grad=0.5856), Var(v=0.2780, grad=-0.9297), Var(v=-0.1080, grad=0.0000), Var(v=0.1344, grad=-0.1916), Var(v=-0.0917, grad=0.1982), Var(v=-0.0717, grad=-0.1260), Var(v=0.2053, grad=-0.3062), Var(v=0.1066, grad=0.0000)], [Var(v=-0.1785, grad=0.0000), Var(v=0.0098, grad=0.4937), Var(v=0.0056, grad=-0.6697), Var(v=0.0647, grad=0.0000), Var(v=-0.1213, grad=0.0000), Var(v=-0.0068, grad=0.6676), Var(v=0.1672, grad=1.6787), Var(v=-0.1034, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0280, grad=0.0000), Var(v=0.0186, grad=-0.4655), Var(v=0.1532, grad=0.0000), Var(v=-0.0072, grad=0.0000), Var(v=-0.1505, grad=-0.5146), Var(v=-0.0371, grad=0.5000), Var(v=-0.1814, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0790, grad=-0.2192), Var(v=-0.1260, grad=-0.1054), Var(v=-0.0873, grad=0.0000), Var(v=0.0913, grad=0.3550), Var(v=0.1548, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.1361, grad=0.3109), Var(v=-0.1112, grad=0.0000), Var(v=-0.0965, grad=0.0000), Var(v=0.2110, grad=0.0000), Var(v=-0.0255, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=-0.0825, grad=0.4175), Var(v=-0.0086, grad=-0.3415), Var(v=0.0817, grad=0.0000), Var(v=-0.0322, grad=-0.3584), Var(v=-0.0309, grad=0.0000), Var(v=0.1137, grad=0.0000), Var(v=0.0189, grad=-0.5903), Var(v=-0.0744, grad=0.0777), Var(v=-0.1882, grad=0.7599), Var(v=0.1145, grad=-0.4924), Var(v=-0.0586, grad=0.0000), Var(v=0.1462, grad=0.4060), Var(v=-0.1243, grad=0.7457), Var(v=-0.0519, grad=-1.1839), Var(v=0.0139, grad=0.0000), Var(v=-0.1989, grad=-0.2439), Var(v=0.0730, grad=0.2524), Var(v=-0.1190, grad=-0.1605), Var(v=-0.0323, grad=-0.3900), Var(v=-0.0711, grad=0.0000)], [Var(v=0.0680, grad=0.0000), Var(v=0.0136, grad=0.8719), Var(v=0.1012, grad=-1.1827), Var(v=-0.1347, grad=0.0000), Var(v=-0.1765, grad=0.0000), Var(v=0.0558, grad=1.1790), Var(v=0.1949, grad=2.9646), Var(v=-0.0265, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=-0.2653, grad=0.0000), Var(v=0.1319, grad=-0.8221), Var(v=0.0243, grad=0.0000), Var(v=-0.0558, grad=0.0000), Var(v=-0.0270, grad=-0.9088), Var(v=-0.0644, grad=0.8830), Var(v=-0.0403, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0849, grad=-0.3870), Var(v=-0.0166, grad=-0.1861), Var(v=-0.1433, grad=0.0000), Var(v=-0.0239, grad=0.6268), Var(v=-0.1179, grad=0.0000), Var(v=0.0957, grad=0.0000), Var(v=-0.0897, grad=0.5490), Var(v=-0.0365, grad=0.0000), Var(v=0.0466, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0515, grad=0.0000), Var(v=-0.0993, grad=0.0000), Var(v=0.0199, grad=0.7373), Var(v=0.0728, grad=-0.6030), Var(v=0.1252, grad=0.0000), Var(v=0.0123, grad=-0.6329), Var(v=0.0019, grad=0.0000), Var(v=0.0220, grad=0.0000), Var(v=0.0202, grad=-1.0424), Var(v=0.0532, grad=0.1373), Var(v=-0.0085, grad=1.3419), Var(v=0.0360, grad=-0.8696), Var(v=-0.0136, grad=0.0000), Var(v=0.0487, grad=0.7169), Var(v=-0.0404, grad=1.3169), Var(v=-0.0777, grad=-2.0907), Var(v=-0.0018, grad=0.0000), Var(v=-0.1071, grad=-0.4308), Var(v=0.0447, grad=0.4457), Var(v=0.0950, grad=-0.2834), Var(v=0.0587, grad=-0.6887), Var(v=-0.0644, grad=0.0000)]] \n",
      " Biases:[Var(v=1.3621, grad=-136.2061), Var(v=-0.0190, grad=1.8956), Var(v=1.2006, grad=-120.0617), Var(v=0.3337, grad=-33.3666), Var(v=1.1955, grad=-119.5546), Var(v=-0.0256, grad=2.5634), Var(v=-0.0645, grad=6.4455), Var(v=-0.7877, grad=78.7737), Var(v=1.9558, grad=-195.5812), Var(v=1.2187, grad=-121.8685), Var(v=0.0000, grad=0.0000), Var(v=0.0179, grad=-1.7874), Var(v=0.5468, grad=-54.6822), Var(v=-0.4357, grad=43.5727), Var(v=0.0198, grad=-1.9759), Var(v=-0.8963, grad=89.6326), Var(v=0.0000, grad=0.0000), Var(v=-2.3844, grad=238.4436), Var(v=0.3929, grad=-39.2869), Var(v=0.0040, grad=-0.4046), Var(v=0.0000, grad=0.0000), Var(v=-0.0136, grad=1.3629), Var(v=0.0000, grad=0.0000), Var(v=0.2477, grad=-24.7699), Var(v=-0.5573, grad=55.7286), Var(v=-0.2533, grad=25.3302), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0160, grad=1.6029), Var(v=0.6121, grad=-61.2135), Var(v=0.5649, grad=-56.4850), Var(v=0.0138, grad=-1.3759), Var(v=-1.5394, grad=153.9375), Var(v=-0.6872, grad=68.7177), Var(v=1.0582, grad=-105.8185), Var(v=-0.0030, grad=0.2985), Var(v=-1.3622, grad=136.2195), Var(v=0.0189, grad=-1.8908), Var(v=0.0000, grad=0.0000), Var(v=-0.7278, grad=72.7763), Var(v=-0.0286, grad=2.8632), Var(v=2.1223, grad=-212.2335), Var(v=0.4092, grad=-40.9236), Var(v=0.4373, grad=-43.7325), Var(v=-0.0097, grad=0.9690), Var(v=0.0062, grad=-0.6162), Var(v=0.0150, grad=-1.4973), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " 50 input nodes \n",
      "1 output nodes \n",
      " Weights:[[Var(v=0.1154, grad=-1.0069)], [Var(v=0.1036, grad=-3.6671)], [Var(v=-0.1233, grad=3.2431)], [Var(v=0.0279, grad=-0.2103)], [Var(v=0.1023, grad=-0.9837)], [Var(v=0.0654, grad=2.5128)], [Var(v=0.2021, grad=2.5585)], [Var(v=-0.0329, grad=-2.8014)], [Var(v=0.1645, grad=-1.3236)], [Var(v=0.1281, grad=-3.3882)], [Var(v=0.1012, grad=0.0000)], [Var(v=-0.0605, grad=-0.2671)], [Var(v=0.0789, grad=-3.6596)], [Var(v=-0.0054, grad=-2.8335)], [Var(v=-0.0253, grad=-4.4556)], [Var(v=0.0484, grad=1.9429)], [Var(v=-0.1217, grad=0.0000)], [Var(v=-0.1817, grad=-0.2703)], [Var(v=-0.0518, grad=2.2048)], [Var(v=0.0180, grad=-3.2260)], [Var(v=-0.0872, grad=0.0000)], [Var(v=0.0761, grad=-2.7974)], [Var(v=0.1191, grad=0.0000)], [Var(v=0.0573, grad=-3.8135)], [Var(v=0.0320, grad=1.0188)], [Var(v=-0.0122, grad=-0.7347)], [Var(v=-0.0579, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0474, grad=0.0000)], [Var(v=0.0069, grad=0.0000)], [Var(v=0.0380, grad=1.8619)], [Var(v=-0.0729, grad=2.6609)], [Var(v=0.0447, grad=-0.1016)], [Var(v=-0.0503, grad=0.1644)], [Var(v=-0.1127, grad=-0.6284)], [Var(v=-0.0322, grad=-2.0964)], [Var(v=-0.0924, grad=1.2349)], [Var(v=0.0349, grad=-2.4381)], [Var(v=0.0978, grad=0.5319)], [Var(v=-0.0671, grad=0.0313)], [Var(v=0.1293, grad=0.0000)], [Var(v=0.0419, grad=1.3201)], [Var(v=0.0978, grad=0.3312)], [Var(v=-0.1777, grad=1.7070)], [Var(v=0.0395, grad=-0.7833)], [Var(v=-0.0481, grad=1.5052)], [Var(v=0.0279, grad=0.6295)], [Var(v=-0.0455, grad=2.3706)], [Var(v=-0.0656, grad=1.2691)], [Var(v=0.2566, grad=0.0000)]] \n",
      " Biases:[Var(v=-0.9435, grad=94.3531)]\n",
      "\n",
      "Network after zeroing gradients:\n",
      "Layer 0 \n",
      " 1 input nodes \n",
      "15 output nodes \n",
      " Weights:[[Var(v=-0.0513, grad=0.0000), Var(v=-0.2216, grad=0.0000), Var(v=-0.0023, grad=0.0000), Var(v=0.0470, grad=0.0000), Var(v=0.2247, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=-0.1275, grad=0.0000), Var(v=-0.1452, grad=0.0000), Var(v=-0.0131, grad=0.0000), Var(v=0.1981, grad=0.0000), Var(v=0.0014, grad=0.0000), Var(v=-0.0407, grad=0.0000), Var(v=-0.1332, grad=0.0000), Var(v=-0.0374, grad=0.0000), Var(v=-0.0825, grad=0.0000)]] \n",
      " Biases:[Var(v=0.0064, grad=0.0000), Var(v=-0.0011, grad=0.0000), Var(v=-0.0042, grad=0.0000), Var(v=-0.0727, grad=0.0000), Var(v=0.0219, grad=0.0000), Var(v=-0.0380, grad=0.0000), Var(v=0.0256, grad=0.0000), Var(v=0.0148, grad=0.0000), Var(v=-0.0337, grad=0.0000), Var(v=0.0421, grad=0.0000), Var(v=-0.0192, grad=0.0000), Var(v=-0.0373, grad=0.0000), Var(v=0.0668, grad=0.0000), Var(v=-0.0351, grad=0.0000), Var(v=-0.0471, grad=0.0000)]\n",
      "Layer 1 \n",
      " 15 input nodes \n",
      "50 output nodes \n",
      " Weights:[[Var(v=0.0328, grad=0.0000), Var(v=0.1673, grad=0.0000), Var(v=0.1016, grad=0.0000), Var(v=-0.0208, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=0.1127, grad=0.0000), Var(v=-0.0033, grad=0.0000), Var(v=-0.0109, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=-0.1918, grad=0.0000), Var(v=0.0553, grad=0.0000), Var(v=-0.0566, grad=0.0000), Var(v=-0.0169, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.1570, grad=0.0000), Var(v=-0.0385, grad=0.0000), Var(v=-0.0611, grad=0.0000), Var(v=0.1277, grad=0.0000), Var(v=-0.1164, grad=0.0000), Var(v=0.0908, grad=0.0000), Var(v=0.0030, grad=0.0000), Var(v=-0.0754, grad=0.0000), Var(v=0.0956, grad=0.0000), Var(v=0.0736, grad=0.0000), Var(v=-0.0288, grad=0.0000), Var(v=0.0931, grad=0.0000), Var(v=-0.0169, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=-0.1294, grad=0.0000), Var(v=-0.2225, grad=0.0000), Var(v=0.0321, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=0.0863, grad=0.0000), Var(v=-0.0130, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.0887, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=-0.0423, grad=0.0000), Var(v=0.1390, grad=0.0000), Var(v=-0.0136, grad=0.0000), Var(v=0.0335, grad=0.0000), Var(v=0.1113, grad=0.0000), Var(v=0.0082, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=0.0279, grad=0.0000), Var(v=0.0953, grad=0.0000), Var(v=0.1564, grad=0.0000), Var(v=0.1456, grad=0.0000), Var(v=0.0514, grad=0.0000)], [Var(v=-0.0939, grad=0.0000), Var(v=-0.0387, grad=0.0000), Var(v=-0.0051, grad=0.0000), Var(v=-0.0833, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=0.1416, grad=0.0000), Var(v=0.0003, grad=0.0000), Var(v=0.1115, grad=0.0000), Var(v=-0.0913, grad=0.0000), Var(v=0.0538, grad=0.0000), Var(v=-0.0794, grad=0.0000), Var(v=-0.1119, grad=0.0000), Var(v=0.0087, grad=0.0000), Var(v=0.0187, grad=0.0000), Var(v=0.1666, grad=0.0000), Var(v=0.0010, grad=0.0000), Var(v=-0.1471, grad=0.0000), Var(v=-0.1155, grad=0.0000), Var(v=0.0724, grad=0.0000), Var(v=0.1082, grad=0.0000), Var(v=-0.1378, grad=0.0000), Var(v=0.0954, grad=0.0000), Var(v=-0.0980, grad=0.0000), Var(v=-0.1332, grad=0.0000), Var(v=-0.0914, grad=0.0000), Var(v=-0.1635, grad=0.0000), Var(v=0.0499, grad=0.0000), Var(v=0.0616, grad=0.0000), Var(v=-0.0631, grad=0.0000), Var(v=-0.1000, grad=0.0000), Var(v=0.1100, grad=0.0000), Var(v=0.1103, grad=0.0000), Var(v=-0.0622, grad=0.0000), Var(v=0.0612, grad=0.0000), Var(v=-0.1276, grad=0.0000), Var(v=-0.0361, grad=0.0000), Var(v=0.0118, grad=0.0000), Var(v=-0.0339, grad=0.0000), Var(v=0.0667, grad=0.0000), Var(v=0.1280, grad=0.0000), Var(v=-0.1227, grad=0.0000), Var(v=-0.1131, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=0.0904, grad=0.0000), Var(v=-0.0981, grad=0.0000), Var(v=0.1680, grad=0.0000), Var(v=0.0746, grad=0.0000), Var(v=0.1309, grad=0.0000), Var(v=0.1112, grad=0.0000), Var(v=-0.0355, grad=0.0000)], [Var(v=0.1000, grad=0.0000), Var(v=-0.2235, grad=0.0000), Var(v=0.0060, grad=0.0000), Var(v=-0.2065, grad=0.0000), Var(v=0.0763, grad=0.0000), Var(v=0.0376, grad=0.0000), Var(v=0.0125, grad=0.0000), Var(v=-0.0677, grad=0.0000), Var(v=-0.0172, grad=0.0000), Var(v=-0.0229, grad=0.0000), Var(v=0.0289, grad=0.0000), Var(v=-0.0684, grad=0.0000), Var(v=-0.0981, grad=0.0000), Var(v=-0.0210, grad=0.0000), Var(v=0.1241, grad=0.0000), Var(v=-0.0512, grad=0.0000), Var(v=-0.0413, grad=0.0000), Var(v=-0.1251, grad=0.0000), Var(v=-0.1129, grad=0.0000), Var(v=0.1228, grad=0.0000), Var(v=-0.0488, grad=0.0000), Var(v=-0.0575, grad=0.0000), Var(v=0.0085, grad=0.0000), Var(v=0.0321, grad=0.0000), Var(v=0.0087, grad=0.0000), Var(v=-0.0862, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=-0.1115, grad=0.0000), Var(v=0.1346, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.1494, grad=0.0000), Var(v=-0.0846, grad=0.0000), Var(v=-0.0192, grad=0.0000), Var(v=-0.1810, grad=0.0000), Var(v=0.0690, grad=0.0000), Var(v=0.0694, grad=0.0000), Var(v=0.0320, grad=0.0000), Var(v=0.0314, grad=0.0000), Var(v=0.1509, grad=0.0000), Var(v=0.0468, grad=0.0000), Var(v=-0.0798, grad=0.0000), Var(v=-0.0177, grad=0.0000), Var(v=-0.1371, grad=0.0000), Var(v=0.0292, grad=0.0000), Var(v=-0.1704, grad=0.0000), Var(v=0.0701, grad=0.0000), Var(v=-0.0394, grad=0.0000), Var(v=0.0768, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=-0.0344, grad=0.0000)], [Var(v=-0.0201, grad=0.0000), Var(v=0.0575, grad=0.0000), Var(v=0.0624, grad=0.0000), Var(v=0.0697, grad=0.0000), Var(v=0.1170, grad=0.0000), Var(v=0.0086, grad=0.0000), Var(v=0.0080, grad=0.0000), Var(v=-0.1907, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=-0.0318, grad=0.0000), Var(v=-0.0267, grad=0.0000), Var(v=-0.0527, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=0.0507, grad=0.0000), Var(v=-0.0046, grad=0.0000), Var(v=0.0273, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.0331, grad=0.0000), Var(v=0.1597, grad=0.0000), Var(v=-0.1270, grad=0.0000), Var(v=0.0320, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=0.1080, grad=0.0000), Var(v=0.2380, grad=0.0000), Var(v=-0.1662, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0885, grad=0.0000), Var(v=0.0763, grad=0.0000), Var(v=-0.1214, grad=0.0000), Var(v=0.0618, grad=0.0000), Var(v=-0.0625, grad=0.0000), Var(v=-0.2101, grad=0.0000), Var(v=0.0282, grad=0.0000), Var(v=-0.0711, grad=0.0000), Var(v=0.0685, grad=0.0000), Var(v=-0.1231, grad=0.0000), Var(v=-0.0713, grad=0.0000), Var(v=-0.0248, grad=0.0000), Var(v=-0.1955, grad=0.0000), Var(v=-0.0490, grad=0.0000), Var(v=0.0349, grad=0.0000), Var(v=0.0703, grad=0.0000), Var(v=-0.0985, grad=0.0000), Var(v=0.0397, grad=0.0000), Var(v=-0.0304, grad=0.0000), Var(v=0.0915, grad=0.0000), Var(v=0.0581, grad=0.0000), Var(v=-0.2703, grad=0.0000), Var(v=-0.0798, grad=0.0000)], [Var(v=0.0368, grad=0.0000), Var(v=0.1058, grad=0.0000), Var(v=-0.1050, grad=0.0000), Var(v=-0.0249, grad=0.0000), Var(v=-0.0052, grad=0.0000), Var(v=0.0084, grad=0.0000), Var(v=0.2188, grad=0.0000), Var(v=0.1365, grad=0.0000), Var(v=0.1056, grad=0.0000), Var(v=0.1623, grad=0.0000), Var(v=-0.0940, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=0.0801, grad=0.0000), Var(v=0.2363, grad=0.0000), Var(v=0.1498, grad=0.0000), Var(v=-0.0533, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0458, grad=0.0000), Var(v=-0.2483, grad=0.0000), Var(v=0.0076, grad=0.0000), Var(v=-0.0282, grad=0.0000), Var(v=0.0307, grad=0.0000), Var(v=-0.0176, grad=0.0000), Var(v=0.1233, grad=0.0000), Var(v=-0.1949, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=-0.0114, grad=0.0000), Var(v=-0.1333, grad=0.0000), Var(v=-0.1367, grad=0.0000), Var(v=0.0176, grad=0.0000), Var(v=0.0471, grad=0.0000), Var(v=0.0776, grad=0.0000), Var(v=-0.0221, grad=0.0000), Var(v=0.0444, grad=0.0000), Var(v=0.0366, grad=0.0000), Var(v=0.1788, grad=0.0000), Var(v=-0.0316, grad=0.0000), Var(v=0.1696, grad=0.0000), Var(v=-0.0994, grad=0.0000), Var(v=0.0405, grad=0.0000), Var(v=-0.0203, grad=0.0000), Var(v=-0.0919, grad=0.0000), Var(v=0.0826, grad=0.0000), Var(v=0.0322, grad=0.0000), Var(v=0.0234, grad=0.0000), Var(v=0.0199, grad=0.0000), Var(v=0.1300, grad=0.0000), Var(v=-0.0174, grad=0.0000), Var(v=-0.0738, grad=0.0000), Var(v=-0.1264, grad=0.0000)], [Var(v=-0.1231, grad=0.0000), Var(v=0.1633, grad=0.0000), Var(v=-0.0221, grad=0.0000), Var(v=-0.0723, grad=0.0000), Var(v=-0.0368, grad=0.0000), Var(v=0.0890, grad=0.0000), Var(v=0.1536, grad=0.0000), Var(v=0.0788, grad=0.0000), Var(v=-0.0538, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.0186, grad=0.0000), Var(v=-0.1466, grad=0.0000), Var(v=-0.0626, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.1086, grad=0.0000), Var(v=0.0890, grad=0.0000), Var(v=0.0728, grad=0.0000), Var(v=-0.0458, grad=0.0000), Var(v=-0.0049, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=-0.0195, grad=0.0000), Var(v=-0.0768, grad=0.0000), Var(v=-0.2029, grad=0.0000), Var(v=-0.0676, grad=0.0000), Var(v=0.0315, grad=0.0000), Var(v=-0.0543, grad=0.0000), Var(v=-0.1141, grad=0.0000), Var(v=-0.0534, grad=0.0000), Var(v=0.0240, grad=0.0000), Var(v=0.1487, grad=0.0000), Var(v=0.1162, grad=0.0000), Var(v=-0.1238, grad=0.0000), Var(v=0.0560, grad=0.0000), Var(v=-0.1621, grad=0.0000), Var(v=-0.0374, grad=0.0000), Var(v=0.1036, grad=0.0000), Var(v=0.1203, grad=0.0000), Var(v=0.0009, grad=0.0000), Var(v=-0.0794, grad=0.0000), Var(v=-0.0151, grad=0.0000), Var(v=0.0882, grad=0.0000), Var(v=-0.0549, grad=0.0000), Var(v=0.0287, grad=0.0000), Var(v=0.2035, grad=0.0000), Var(v=-0.0819, grad=0.0000), Var(v=0.0748, grad=0.0000), Var(v=0.1000, grad=0.0000), Var(v=-0.1468, grad=0.0000), Var(v=0.0025, grad=0.0000), Var(v=-0.1711, grad=0.0000)], [Var(v=0.1576, grad=0.0000), Var(v=0.0972, grad=0.0000), Var(v=-0.0222, grad=0.0000), Var(v=0.0114, grad=0.0000), Var(v=0.0987, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=-0.0231, grad=0.0000), Var(v=-0.1075, grad=0.0000), Var(v=0.0704, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=-0.0807, grad=0.0000), Var(v=0.2528, grad=0.0000), Var(v=0.0996, grad=0.0000), Var(v=-0.0386, grad=0.0000), Var(v=-0.0534, grad=0.0000), Var(v=0.1363, grad=0.0000), Var(v=-0.0282, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=0.1121, grad=0.0000), Var(v=-0.0398, grad=0.0000), Var(v=-0.0429, grad=0.0000), Var(v=0.0565, grad=0.0000), Var(v=0.1357, grad=0.0000), Var(v=-0.0746, grad=0.0000), Var(v=-0.0580, grad=0.0000), Var(v=0.1266, grad=0.0000), Var(v=-0.1812, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0999, grad=0.0000), Var(v=0.1369, grad=0.0000), Var(v=-0.1409, grad=0.0000), Var(v=0.1803, grad=0.0000), Var(v=0.0579, grad=0.0000), Var(v=0.1533, grad=0.0000), Var(v=-0.0895, grad=0.0000), Var(v=0.0481, grad=0.0000), Var(v=0.1003, grad=0.0000), Var(v=0.0869, grad=0.0000), Var(v=0.0571, grad=0.0000), Var(v=-0.1137, grad=0.0000), Var(v=0.1125, grad=0.0000), Var(v=0.1120, grad=0.0000), Var(v=-0.1037, grad=0.0000), Var(v=0.0539, grad=0.0000), Var(v=-0.0428, grad=0.0000), Var(v=-0.0314, grad=0.0000), Var(v=0.0105, grad=0.0000), Var(v=0.1240, grad=0.0000), Var(v=0.0431, grad=0.0000), Var(v=0.0096, grad=0.0000)], [Var(v=-0.0274, grad=0.0000), Var(v=-0.2498, grad=0.0000), Var(v=0.0987, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.1204, grad=0.0000), Var(v=-0.0229, grad=0.0000), Var(v=-0.0662, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.1152, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.0927, grad=0.0000), Var(v=-0.1886, grad=0.0000), Var(v=-0.1123, grad=0.0000), Var(v=0.0221, grad=0.0000), Var(v=-0.0284, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0637, grad=0.0000), Var(v=-0.0798, grad=0.0000), Var(v=0.0785, grad=0.0000), Var(v=-0.1396, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=-0.0128, grad=0.0000), Var(v=0.2178, grad=0.0000), Var(v=0.0906, grad=0.0000), Var(v=-0.1521, grad=0.0000), Var(v=-0.2826, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1186, grad=0.0000), Var(v=0.0861, grad=0.0000), Var(v=-0.0076, grad=0.0000), Var(v=-0.1232, grad=0.0000), Var(v=0.0740, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.1028, grad=0.0000), Var(v=-0.0655, grad=0.0000), Var(v=0.0889, grad=0.0000), Var(v=-0.0081, grad=0.0000), Var(v=0.1007, grad=0.0000), Var(v=0.0192, grad=0.0000), Var(v=-0.1040, grad=0.0000), Var(v=0.2035, grad=0.0000), Var(v=0.0298, grad=0.0000), Var(v=0.0582, grad=0.0000), Var(v=0.0656, grad=0.0000), Var(v=-0.1463, grad=0.0000), Var(v=0.0481, grad=0.0000), Var(v=-0.1873, grad=0.0000), Var(v=-0.1961, grad=0.0000)], [Var(v=-0.0516, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=-0.0786, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.0813, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.1434, grad=0.0000), Var(v=-0.0927, grad=0.0000), Var(v=0.0590, grad=0.0000), Var(v=0.0556, grad=0.0000), Var(v=0.0712, grad=0.0000), Var(v=-0.0188, grad=0.0000), Var(v=-0.0017, grad=0.0000), Var(v=-0.0929, grad=0.0000), Var(v=0.0664, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.1503, grad=0.0000), Var(v=0.1185, grad=0.0000), Var(v=0.0351, grad=0.0000), Var(v=-0.0620, grad=0.0000), Var(v=0.1342, grad=0.0000), Var(v=-0.0941, grad=0.0000), Var(v=0.0493, grad=0.0000), Var(v=0.0351, grad=0.0000), Var(v=-0.0448, grad=0.0000), Var(v=-0.0870, grad=0.0000), Var(v=0.0783, grad=0.0000), Var(v=0.0144, grad=0.0000), Var(v=0.0496, grad=0.0000), Var(v=-0.1935, grad=0.0000), Var(v=0.0116, grad=0.0000), Var(v=-0.1522, grad=0.0000), Var(v=-0.0421, grad=0.0000), Var(v=0.0179, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=-0.2378, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=0.0513, grad=0.0000), Var(v=-0.1068, grad=0.0000), Var(v=0.0563, grad=0.0000), Var(v=-0.0248, grad=0.0000), Var(v=0.0133, grad=0.0000), Var(v=0.0870, grad=0.0000), Var(v=0.0107, grad=0.0000), Var(v=-0.1085, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=0.1127, grad=0.0000), Var(v=-0.0614, grad=0.0000)], [Var(v=0.0697, grad=0.0000), Var(v=0.1562, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.0760, grad=0.0000), Var(v=0.0986, grad=0.0000), Var(v=0.0456, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=-0.0447, grad=0.0000), Var(v=0.0277, grad=0.0000), Var(v=0.0394, grad=0.0000), Var(v=0.1086, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=0.1604, grad=0.0000), Var(v=-0.1398, grad=0.0000), Var(v=0.1415, grad=0.0000), Var(v=0.0151, grad=0.0000), Var(v=-0.0092, grad=0.0000), Var(v=-0.1210, grad=0.0000), Var(v=0.0089, grad=0.0000), Var(v=0.2101, grad=0.0000), Var(v=0.0081, grad=0.0000), Var(v=0.2602, grad=0.0000), Var(v=-0.0009, grad=0.0000), Var(v=0.1141, grad=0.0000), Var(v=-0.0850, grad=0.0000), Var(v=0.1112, grad=0.0000), Var(v=0.0055, grad=0.0000), Var(v=0.0415, grad=0.0000), Var(v=0.0248, grad=0.0000), Var(v=-0.0288, grad=0.0000), Var(v=-0.0353, grad=0.0000), Var(v=-0.0815, grad=0.0000), Var(v=0.0530, grad=0.0000), Var(v=0.1977, grad=0.0000), Var(v=-0.0509, grad=0.0000), Var(v=-0.1256, grad=0.0000), Var(v=-0.0673, grad=0.0000), Var(v=0.0779, grad=0.0000), Var(v=0.1317, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=-0.0143, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=0.0098, grad=0.0000), Var(v=-0.1060, grad=0.0000), Var(v=0.0222, grad=0.0000), Var(v=-0.1463, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=0.0396, grad=0.0000), Var(v=0.0909, grad=0.0000), Var(v=0.1110, grad=0.0000)], [Var(v=0.2044, grad=0.0000), Var(v=0.1642, grad=0.0000), Var(v=0.0626, grad=0.0000), Var(v=-0.0533, grad=0.0000), Var(v=-0.0691, grad=0.0000), Var(v=-0.0981, grad=0.0000), Var(v=0.0843, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=0.0033, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0659, grad=0.0000), Var(v=0.1348, grad=0.0000), Var(v=0.0647, grad=0.0000), Var(v=-0.0987, grad=0.0000), Var(v=0.1888, grad=0.0000), Var(v=0.0384, grad=0.0000), Var(v=-0.0899, grad=0.0000), Var(v=0.0044, grad=0.0000), Var(v=-0.2462, grad=0.0000), Var(v=-0.1811, grad=0.0000), Var(v=0.0490, grad=0.0000), Var(v=0.0052, grad=0.0000), Var(v=-0.0139, grad=0.0000), Var(v=0.0396, grad=0.0000), Var(v=0.1291, grad=0.0000), Var(v=-0.1627, grad=0.0000), Var(v=-0.0507, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=0.1070, grad=0.0000), Var(v=-0.1748, grad=0.0000), Var(v=-0.0323, grad=0.0000), Var(v=-0.0697, grad=0.0000), Var(v=-0.1400, grad=0.0000), Var(v=0.0276, grad=0.0000), Var(v=0.0108, grad=0.0000), Var(v=-0.2101, grad=0.0000), Var(v=-0.2146, grad=0.0000), Var(v=-0.0078, grad=0.0000), Var(v=-0.1981, grad=0.0000), Var(v=0.1132, grad=0.0000), Var(v=0.0550, grad=0.0000), Var(v=-0.0971, grad=0.0000), Var(v=0.0693, grad=0.0000), Var(v=0.0700, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=0.0942, grad=0.0000), Var(v=-0.0608, grad=0.0000), Var(v=0.0973, grad=0.0000), Var(v=-0.0521, grad=0.0000)], [Var(v=-0.1435, grad=0.0000), Var(v=0.0001, grad=0.0000), Var(v=0.1072, grad=0.0000), Var(v=-0.2017, grad=0.0000), Var(v=0.1438, grad=0.0000), Var(v=0.0014, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=-0.0296, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=0.0540, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0533, grad=0.0000), Var(v=0.0883, grad=0.0000), Var(v=0.1265, grad=0.0000), Var(v=-0.0277, grad=0.0000), Var(v=-0.0051, grad=0.0000), Var(v=-0.1604, grad=0.0000), Var(v=0.0026, grad=0.0000), Var(v=0.1238, grad=0.0000), Var(v=-0.0121, grad=0.0000), Var(v=-0.1292, grad=0.0000), Var(v=0.0951, grad=0.0000), Var(v=-0.0757, grad=0.0000), Var(v=0.1149, grad=0.0000), Var(v=0.2048, grad=0.0000), Var(v=-0.2998, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0416, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=0.1138, grad=0.0000), Var(v=-0.1478, grad=0.0000), Var(v=-0.1122, grad=0.0000), Var(v=0.1357, grad=0.0000), Var(v=0.0861, grad=0.0000), Var(v=-0.0581, grad=0.0000), Var(v=0.0235, grad=0.0000), Var(v=0.0292, grad=0.0000), Var(v=-0.0205, grad=0.0000), Var(v=-0.0856, grad=0.0000), Var(v=-0.0582, grad=0.0000), Var(v=0.0886, grad=0.0000), Var(v=0.0343, grad=0.0000), Var(v=-0.0389, grad=0.0000), Var(v=-0.0017, grad=0.0000), Var(v=-0.0701, grad=0.0000), Var(v=0.0591, grad=0.0000), Var(v=-0.2088, grad=0.0000), Var(v=0.0264, grad=0.0000), Var(v=0.0534, grad=0.0000)], [Var(v=0.0445, grad=0.0000), Var(v=0.0321, grad=0.0000), Var(v=0.0776, grad=0.0000), Var(v=0.0215, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=-0.0682, grad=0.0000), Var(v=0.0192, grad=0.0000), Var(v=0.1279, grad=0.0000), Var(v=-0.0392, grad=0.0000), Var(v=0.0804, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=-0.0538, grad=0.0000), Var(v=-0.1589, grad=0.0000), Var(v=-0.0182, grad=0.0000), Var(v=-0.0583, grad=0.0000), Var(v=0.1930, grad=0.0000), Var(v=0.0346, grad=0.0000), Var(v=0.2010, grad=0.0000), Var(v=-0.0470, grad=0.0000), Var(v=0.0737, grad=0.0000), Var(v=0.0323, grad=0.0000), Var(v=-0.1276, grad=0.0000), Var(v=-0.0293, grad=0.0000), Var(v=0.0097, grad=0.0000), Var(v=-0.0162, grad=0.0000), Var(v=0.1279, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=0.0131, grad=0.0000), Var(v=-0.0648, grad=0.0000), Var(v=0.0968, grad=0.0000), Var(v=-0.0980, grad=0.0000), Var(v=0.0109, grad=0.0000), Var(v=-0.0497, grad=0.0000), Var(v=0.0274, grad=0.0000), Var(v=0.0367, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.1774, grad=0.0000), Var(v=-0.0035, grad=0.0000), Var(v=-0.0359, grad=0.0000), Var(v=0.0429, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=0.1452, grad=0.0000), Var(v=-0.1087, grad=0.0000), Var(v=0.2780, grad=0.0000), Var(v=-0.1080, grad=0.0000), Var(v=0.1344, grad=0.0000), Var(v=-0.0917, grad=0.0000), Var(v=-0.0717, grad=0.0000), Var(v=0.2053, grad=0.0000), Var(v=0.1066, grad=0.0000)], [Var(v=-0.1785, grad=0.0000), Var(v=0.0098, grad=0.0000), Var(v=0.0056, grad=0.0000), Var(v=0.0647, grad=0.0000), Var(v=-0.1213, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=0.1672, grad=0.0000), Var(v=-0.1034, grad=0.0000), Var(v=-0.0071, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0280, grad=0.0000), Var(v=0.0186, grad=0.0000), Var(v=0.1532, grad=0.0000), Var(v=-0.0072, grad=0.0000), Var(v=-0.1505, grad=0.0000), Var(v=-0.0371, grad=0.0000), Var(v=-0.1814, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=0.0790, grad=0.0000), Var(v=-0.1260, grad=0.0000), Var(v=-0.0873, grad=0.0000), Var(v=0.0913, grad=0.0000), Var(v=0.1548, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.1361, grad=0.0000), Var(v=-0.1112, grad=0.0000), Var(v=-0.0965, grad=0.0000), Var(v=0.2110, grad=0.0000), Var(v=-0.0255, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=-0.0825, grad=0.0000), Var(v=-0.0086, grad=0.0000), Var(v=0.0817, grad=0.0000), Var(v=-0.0322, grad=0.0000), Var(v=-0.0309, grad=0.0000), Var(v=0.1137, grad=0.0000), Var(v=0.0189, grad=0.0000), Var(v=-0.0744, grad=0.0000), Var(v=-0.1882, grad=0.0000), Var(v=0.1145, grad=0.0000), Var(v=-0.0586, grad=0.0000), Var(v=0.1462, grad=0.0000), Var(v=-0.1243, grad=0.0000), Var(v=-0.0519, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=-0.1989, grad=0.0000), Var(v=0.0730, grad=0.0000), Var(v=-0.1190, grad=0.0000), Var(v=-0.0323, grad=0.0000), Var(v=-0.0711, grad=0.0000)], [Var(v=0.0680, grad=0.0000), Var(v=0.0136, grad=0.0000), Var(v=0.1012, grad=0.0000), Var(v=-0.1347, grad=0.0000), Var(v=-0.1765, grad=0.0000), Var(v=0.0558, grad=0.0000), Var(v=0.1949, grad=0.0000), Var(v=-0.0265, grad=0.0000), Var(v=-0.0793, grad=0.0000), Var(v=-0.1024, grad=0.0000), Var(v=-0.2653, grad=0.0000), Var(v=0.1319, grad=0.0000), Var(v=0.0243, grad=0.0000), Var(v=-0.0558, grad=0.0000), Var(v=-0.0270, grad=0.0000), Var(v=-0.0644, grad=0.0000), Var(v=-0.0403, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0849, grad=0.0000), Var(v=-0.0166, grad=0.0000), Var(v=-0.1433, grad=0.0000), Var(v=-0.0239, grad=0.0000), Var(v=-0.1179, grad=0.0000), Var(v=0.0957, grad=0.0000), Var(v=-0.0897, grad=0.0000), Var(v=-0.0365, grad=0.0000), Var(v=0.0466, grad=0.0000), Var(v=-0.0694, grad=0.0000), Var(v=-0.0515, grad=0.0000), Var(v=-0.0993, grad=0.0000), Var(v=0.0199, grad=0.0000), Var(v=0.0728, grad=0.0000), Var(v=0.1252, grad=0.0000), Var(v=0.0123, grad=0.0000), Var(v=0.0019, grad=0.0000), Var(v=0.0220, grad=0.0000), Var(v=0.0202, grad=0.0000), Var(v=0.0532, grad=0.0000), Var(v=-0.0085, grad=0.0000), Var(v=0.0360, grad=0.0000), Var(v=-0.0136, grad=0.0000), Var(v=0.0487, grad=0.0000), Var(v=-0.0404, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=-0.1071, grad=0.0000), Var(v=0.0447, grad=0.0000), Var(v=0.0950, grad=0.0000), Var(v=0.0587, grad=0.0000), Var(v=-0.0644, grad=0.0000)]] \n",
      " Biases:[Var(v=1.3621, grad=0.0000), Var(v=-0.0190, grad=0.0000), Var(v=1.2006, grad=0.0000), Var(v=0.3337, grad=0.0000), Var(v=1.1955, grad=0.0000), Var(v=-0.0256, grad=0.0000), Var(v=-0.0645, grad=0.0000), Var(v=-0.7877, grad=0.0000), Var(v=1.9558, grad=0.0000), Var(v=1.2187, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0179, grad=0.0000), Var(v=0.5468, grad=0.0000), Var(v=-0.4357, grad=0.0000), Var(v=0.0198, grad=0.0000), Var(v=-0.8963, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-2.3844, grad=0.0000), Var(v=0.3929, grad=0.0000), Var(v=0.0040, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0136, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.2477, grad=0.0000), Var(v=-0.5573, grad=0.0000), Var(v=-0.2533, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.0160, grad=0.0000), Var(v=0.6121, grad=0.0000), Var(v=0.5649, grad=0.0000), Var(v=0.0138, grad=0.0000), Var(v=-1.5394, grad=0.0000), Var(v=-0.6872, grad=0.0000), Var(v=1.0582, grad=0.0000), Var(v=-0.0030, grad=0.0000), Var(v=-1.3622, grad=0.0000), Var(v=0.0189, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=-0.7278, grad=0.0000), Var(v=-0.0286, grad=0.0000), Var(v=2.1223, grad=0.0000), Var(v=0.4092, grad=0.0000), Var(v=0.4373, grad=0.0000), Var(v=-0.0097, grad=0.0000), Var(v=0.0062, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " 50 input nodes \n",
      "1 output nodes \n",
      " Weights:[[Var(v=0.1154, grad=0.0000)], [Var(v=0.1036, grad=0.0000)], [Var(v=-0.1233, grad=0.0000)], [Var(v=0.0279, grad=0.0000)], [Var(v=0.1023, grad=0.0000)], [Var(v=0.0654, grad=0.0000)], [Var(v=0.2021, grad=0.0000)], [Var(v=-0.0329, grad=0.0000)], [Var(v=0.1645, grad=0.0000)], [Var(v=0.1281, grad=0.0000)], [Var(v=0.1012, grad=0.0000)], [Var(v=-0.0605, grad=0.0000)], [Var(v=0.0789, grad=0.0000)], [Var(v=-0.0054, grad=0.0000)], [Var(v=-0.0253, grad=0.0000)], [Var(v=0.0484, grad=0.0000)], [Var(v=-0.1217, grad=0.0000)], [Var(v=-0.1817, grad=0.0000)], [Var(v=-0.0518, grad=0.0000)], [Var(v=0.0180, grad=0.0000)], [Var(v=-0.0872, grad=0.0000)], [Var(v=0.0761, grad=0.0000)], [Var(v=0.1191, grad=0.0000)], [Var(v=0.0573, grad=0.0000)], [Var(v=0.0320, grad=0.0000)], [Var(v=-0.0122, grad=0.0000)], [Var(v=-0.0579, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0474, grad=0.0000)], [Var(v=0.0069, grad=0.0000)], [Var(v=0.0380, grad=0.0000)], [Var(v=-0.0729, grad=0.0000)], [Var(v=0.0447, grad=0.0000)], [Var(v=-0.0503, grad=0.0000)], [Var(v=-0.1127, grad=0.0000)], [Var(v=-0.0322, grad=0.0000)], [Var(v=-0.0924, grad=0.0000)], [Var(v=0.0349, grad=0.0000)], [Var(v=0.0978, grad=0.0000)], [Var(v=-0.0671, grad=0.0000)], [Var(v=0.1293, grad=0.0000)], [Var(v=0.0419, grad=0.0000)], [Var(v=0.0978, grad=0.0000)], [Var(v=-0.1777, grad=0.0000)], [Var(v=0.0395, grad=0.0000)], [Var(v=-0.0481, grad=0.0000)], [Var(v=0.0279, grad=0.0000)], [Var(v=-0.0455, grad=0.0000)], [Var(v=-0.0656, grad=0.0000)], [Var(v=0.2566, grad=0.0000)]] \n",
      " Biases:[Var(v=-0.9435, grad=0.0000)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Network before update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "def parameters(network):\n",
    "    params = []\n",
    "    for layer in range(len(network)):\n",
    "        params += network[layer].parameters()\n",
    "    return params\n",
    "\n",
    "def update_parameters(params, learning_rate=0.01):\n",
    "    for p in params:\n",
    "        p.v -= learning_rate*p.grad\n",
    "\n",
    "def zero_gradients(params):\n",
    "    for p in params:\n",
    "        p.grad = 0.0\n",
    "\n",
    "update_parameters(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "zero_gradients(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after zeroing gradients:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "woWYpdw6FtIO"
   },
   "outputs": [],
   "source": [
    "# Initialize an arbitrary neural network\n",
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "# Recommended hyper-parameters for 3-D: \n",
    "#NN = [\n",
    "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
    "#    DenseLayer(16, 1, lambda x: x.identity())\n",
    "#]\n",
    "\n",
    "\n",
    "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
    "## of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdqaqYBVFtIR"
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = 600\n",
    "LEARN_R = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kfg76GMFtIW",
    "outputId": "e30cf68a-31f2-42b4-cc5e-860c297c0f04"
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation,forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "VetyRWFwFtIY",
    "outputId": "344e490d-6d7d-455a-fa6f-88dd11eb957e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss);\n",
    "plt.plot(range(len(val_loss)), val_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OgmIrM9FtIb"
   },
   "source": [
    "# Testing\n",
    "\n",
    "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmNi7S-vFtIc"
   },
   "outputs": [],
   "source": [
    "output_test = forward(x_test, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "7mmJOTSEFtIf",
    "outputId": "e3264095-cefe-4aee-893d-bf152438e332"
   },
   "outputs": [],
   "source": [
    "y_test_np = Var_to_nparray(y_test)\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ODi0WlmQFtIh",
    "outputId": "d1ab874f-0717-4987-87bf-1f0c7c8e7148"
   },
   "outputs": [],
   "source": [
    "x_test_np = Var_to_nparray(x_test)\n",
    "x_train_np = Var_to_nparray(x_train)\n",
    "y_train_np = Var_to_nparray(y_train)\n",
    "if D1:\n",
    "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
    "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
    "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");\n",
    "else:\n",
    "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
    "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
    "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTBAmjsAFtIk"
   },
   "source": [
    "## Exercise k) Show overfitting, underfitting and just right fitting\n",
    "\n",
    "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
    "\n",
    "See also if you can get a good compromise which leads to a low validation loss. \n",
    "\n",
    "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
    "\n",
    "_Insert written answer here._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For separating the validation dataset and test dataset, it is mainly because the use of these two datasets is different.\n",
    "\n",
    "For validation dataset, we focus on building model, it means adjusting hyper parameters, like epoches, learning rate, nodes or so and avoid some accidents, like gradient explosion, to draw the model back into right way. \n",
    "\n",
    "For test dataset, we focus on testing model, it means the genralization ability of this model need to be test. In this dataset, we just test the performance of the model but do not adjust it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 ( 0.00%) Train loss: 106.206 \t Validation loss: 108.544\n",
      "  10 (15.15%) Train loss: 105.583 \t Validation loss: 108.151\n",
      "  20 (30.30%) Train loss: 104.985 \t Validation loss: 107.556\n",
      "  30 (45.45%) Train loss: 104.400 \t Validation loss: 106.972\n",
      "  40 (60.61%) Train loss: 103.823 \t Validation loss: 106.396\n",
      "  50 (75.76%) Train loss: 103.258 \t Validation loss: 105.831\n",
      "  60 (90.91%) Train loss: 102.711 \t Validation loss: 105.282\n",
      "_______________________________________________________________\n",
      "Train loss:  102.447\n",
      "Test loss:  97.414\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAAEYCAYAAADCo4ZLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAikklEQVR4nO3de5QcdZ338fd3JhOYEDRgcJRJSBRYCBIEJgt4eNxNECUgAvLIg6y64C3remMeUZHLs4KSA7soigeVVXBBkQR2TVhF3RB0RlYUJYFAkp1BkcvkQuQaIRAlJN/nj/r1UNPT1d0z6e6q6v68zpmT6a6uX33711W/T916Yu6OiIhIq2tLuwAREZEsUCCKiIigQBQREQEUiCIiIoACUUREBFAgioiIADkLRDObaWZuZhOqeO1ZZvbLRtRVYtkj6jSzn5rZmeNoZx8z22Jm7bWvMntCn+2XMG1cfZjQVtXrkUTMbK6ZrW/Ach4xs2PrvZx6Krcej7Gdmowj41x2v5l9qEZt5eYzrVsghk540cymFj2/KnzIM+u17Kxx9+Pd/fpKrytecdx9yN0nu/v2+laYfdX2YSn13CBrGRS1HISK2h3zzmGtBvWsM7PrzOySnZi/Lp9ZKTuzDZRjZheZ2Q21brcWirfdeu/M1vsI8WHgjMIDM5sNdNZ5mTWnI4nqtMqRrIjkU8Wx3N3r8gM8AlwI3B177kvABYADM8NzrwS+CzwBPBrmaQvT2sM8TwIPAR8L806IzXst8BiwAbgEaA/TzgJ+mVDbzNDOAmBjmP+c2PSLgP8AbgCeBT5UYVmV6uwHPhRr/8PAAPAc8D/A4cD3gB3AVmAL8NlYnYV29gZ+CDwNPAh8uKjmm0NfPgesBeYkvP+rgS8VPfefwKfC7+eG9/gc8ADwloR2rgO+CfwEeB44NtT4g/B5Pgx8Mvb6I4BfA5tDP14FTIxNd2C/hGUN92Hhsw19/kxYzvEJ85Xr1zOBofC5XRCbpw34HPAH4KnQr3uWaHu30O6O0PaW8P4T5wd2JVqvngr9cDfQBSwEtgN/Du1cVWJ5Jectty0As0Kb20O7m6vYdu8I/fN8mOd0YC6wHjgHeDws5/2xeXYJn8cQ8EeidayzzDJGbQOxcePYWJtfJdpGN4bfdwnTpgK3hn54GvhvXh43EtfBohoWANuAF8P7/FF4fhbR+raZaDs6KWH+kp9Z6LuPAL8nWj+/Dlhsvg+E9/4MsAyYUWGcGjWOUGEbSFofSixjfnj/28J7uC+2rC8Cd4bP6DZgamy+o4BfhT66D5hbIQvOC5/zM8C/AbvGpp8IrApt/Qo4pMy2OxT6pLC9valSn4bXfyx8Hg+XXfcrbRzj/QmdcCzRgDqLaONcB8xgZCB+l2gw3j2sAL8DPhimfQQYBKYDewJ9RSvILcC/Eg1MrwZ+C/xDfIWpsKItCvPOJtp4ChviRWEFOYVocOussKxKdfbz8op8GtEK+teAAfsVPjxig0HCBvEL4BtEA+Ohoea3xGr+M3BC6OtLgbsS3v/fhM/CwuM9iFa6vYEDwrS9YzXsm9DOdcCfgKNDP00CVgL/BEwEXk+0g3BceH0P0YY0IbQ7APQWrbjVBuI2okG1HfhHogHTEuZN6tdvh8/2jcBfgFlhei9wFzCNaFD+V2BRQttzgfVFzyXOD/wD8KPQV+2hT15R/B4TllVu3lsYx7ZQZlkjPovwPl8CvgB0hPXsBWCPMP2rRDtrexJtyz8CLk1ou6ptICzrrvB+9iIaLL8Ypl1KFLod4efNoa02yqyDCevwJbHHHUQ7m+eH+Y8hCoQDKq2XRX13KzAF2IdoO50fpp0S2p9FtB1cCPyqwjiVFIiJ20C59aHEci4Cbijxvv4A/BXRNtIPXBamdRPtlJ0Q+vut4fFeZba/Nbw8Pt5Z6HOig4HHgSPD+zgzvH6X2LyJY2I1fRpevzwsO3Enzb0xgXgh0co7PxQ1IRQ4M3TAX4CDijb6/vD7z4GPxKa9rdAZRHvVf4m/QaLTs32VBoFYpx4Ye+5fgGtjK8gdsWmVlpVYZ4kVeRlwdrk+K/Xhh5VpO7B7bPqlwHWxmm+PTTsI2JqwHCPa0/qb8PjDwM/D7/uFFfRYoKPCZ3wd8N3Y4yOBoaLXnAf8W8L8vcDSohW32kB8MDZtUpj3NWPs12mx534LvDv8PkDsqBh4LdHgM6FE23MZHYiJ8xPtyQ7vBSe9x4T3UXLeKtbPs6hNIG5l5ED0ONEOjhEdTe4bm/YmEvbGqXIbIBqQT4hNOw54JPz+BaId6f2K5h/rOngdIwPxzcAmwtFmeG4RcFGl9bKo7/5X7PHNwOfC7z8l7PCHx21EOxYzSrRdWE+TArHkNlBpfSixnIsoHYgXxh5/FPiv8Pu5wPdKfKZnlvlM4+PjCcAfwu/fJOzkxKY/APxt8fpQqk+q6dPw+mOqWe8bcW3se0SnYF5HdDQYN5VoL+zR2HOPEu2BQHTEsq5oWsEMor25x8ys8Fxb0esrKW57dsK0SssqV2ex6UQb+ljtDTzt7s8VLWdO7PGm2O8vALua2QR3fynekLu7mS0m2kjuAP6O6FQc7v6gmfUSbSRvMLNlRKdSNybUVdxPe5vZ5thz7USnszCzvwKuCDVPIgqIleXfdqLh9+ruL4TPZfJ42yDqr8L8M4ClZrYjNn070UCzoYp2y83/PaJ1YLGZTSHq9wvcfVsV7Zacl9psC9V4qmhdKvTZXoSzA7HlG9FnX0q128DejB4b9g6/X060jt4Wlvktd7+MCutglctc5+7xzy4+JlWr3Lp1pZl9OTbdQvvlxo2yyyjaBvakNutDufdwmpm9Iza9g+jMWJLi8bHwOc4AzjSzT8SmT4xNr0Y1fVrVe697ILr7o2b2MNFewQeLJj9JtOc8g+j8MkSnGAqDzmNEGw+xaQXriPaCphYP+GMwnehUZ6Ht+KDvY1hWuTqLrQP2TZjmCc8TatvTzHaPhWK8r8ZqEdFgchnRXvU7h4twvxG40cxeQXTa5Z+B91VR8zqio4L9E177TeBe4Ax3fy4E77vGWf9YlOvXUtYBH3D3O8fZdqX5LwYuDnda/4Roj/jaSnWG0Cw1708ov36O9f2P1ZNER49vcPdq1sdy20DcRqKxYW14PLyNhm3gHOAcM3sD0Gdmd1N5HSxW3Dcbgelm1hYLxX2ILuVUM38l64CF7v79Mc431mWMZWwcz3v4nrt/eAzzFI+PhbG20B8Lq6wtaXur1KdVvcdGfQ/xg0SHrM/Hn/To6wQ3AwvNbHczmwF8inC0EqZ90symmdkeRDcqFOZ9jOhC75fN7BVm1mZm+5rZ346hrv9nZpPCBvV+4KZSL6piWYl1lnAN8Gkz67HIfuF9Q3QzwusTalhHdLrsUjPb1cwOIerXcW1Y7n4v0bWNa4Bl7r4ZwMwOMLNjzGwXomuSW4mObqrxW+BZMzvXzDrNrN3MDjazvw7Tdye6SWmLmR1IdN2jERL7NcHVROvkDAAz28vMTi7T9qvM7JXVzG9m88xsdrgj91miHcLtsbYS60yat4r184/ANDObGGvrLDN7pEwfVN1nITi+DXzFzF4d2u82s+MSZim3DcQtAi4M/TeV6LrgDaH9E8N8Fvpie/iptA5Wep+/ITr9+1kz6zCzucA7gMVVzl/J1cB5YczBzF5pZqeNYf6KxjE2/hGYaWbV5sENwDvM7LjQv7ta9PWjaWXm+VgYH/ckuj5bGGu/DXzEzI4M68JuZvZ2M9s9Vlu8f58gutEm/lzN+rQhgejuf3D3FQmTP0G0Aj5EdNfUjcB3wrRvE52bvg+4B1hSNO/fEx1eF+5e+g+i6zXV+gXRxdifEd11eVuZ15ZbVqU6h7n7vxPdnXYj0cX6W4hOcUB0TfBCM9tsZp8uMfsZROfQNwJLgc+7+/JKb7KMRUTXCm+MPbcLcBnRXv8mogvy51fTWNjBeQfRDT8PhzauIbrjDeDTRKdnnyPqs5I7IHVQqV+LXUl0g8htZvYc0Y0dR5Z6obsPEvXjQ6H9vSvM/xqidedZomuNv+DlHcArgXeZ2TNm9rUSiys3b7n18+dER1mbzOzJ8Nx0opsbklwEXB/e0/8p87qCc4m2pbvM7FngdqIbtEapsA3EXQKsAO4HVhNtW4XvDO4flrGF6M7lb7h7fxXrYLFrgYPC+7zF3V8ETgKOD/N+A/j78DmXUukzK37vS4nOuCwO/bQmLKvWxjI2/nv49ykzu6dSw2Hn/GSiceEJoiO0z1A+T24kCumHws8loa0VRPcwXBXqfJDo+mjBiG3X3V8gWnfuDM8dVcs+LdyR1FIsOt30MNFNI+M93SqSW2Z2G9GNLQNp1yKSFfrCuUgLcve3pV2DSNbk6m+ZioiI1EtLnjIVEREppiNEERERmuQa4tSpU33mzJmJ059//nl22223xhW0k1RvfeWtXshfzaq3vpqt3pUrVz7p7ns1sKTSqvlzNln/6enp8XL6+vrKTs8a1VtfeavXPX81q976arZ6gRWegSzRKVMRERF0DVFERARQIIqIiAAKRBEREUCBKCIiAigQRUREAAWiiIgIoEAUEana4OAgS5Yk/u9uknNN8ZdqRETqbXBwkLlz59LR0cHxxx9PZ2dn2iVJjekIUUSkgkIYAixfvlxh2KQUiCIiZcTDsL+/nwMPPDDdgqRuFIgiIgkUhq1FgSgiUoLCsPUoEEVEiigMW5MCUUQkRmHYuhSIIiKBwrC1KRBFRFAYSkYD0cymm1mfmQ2Y2VozOzvtmkSkeQ0NDSkMJZuBCLwEnOPus4CjgI+Z2UEp1yQiTWhwcJDe3l5AYdjqMhmI7v6Yu98Tfn8OGAC6061KRJqNTpNKnLl72jWUZWYzgTuAg9392djzC4AFAF1dXT2LFy9ObGPLli1Mnjy5zpXWjuqtr7zVC/mrOQ/1Dg0NDR8ZLly4kFmzZqVb0BjkoX/jKtU7b968le4+p4Ellebumf0BJgMrgVPLva6np8fL6evrKzs9a1RvfeWtXvf81Zz1egcGBryrq8u7urp8YGAg8/UWa7Z6gRWegczJ5ClTADPrAH4AfN/d9f+tiEhN6DSpJMlkIJqZAdcCA+5+Rdr1iEhzUBhKOZkMROBo4H3AMWa2KvyckHZRIpJfCkOpJJP/QbC7/xKwtOsQkeagMJRqZPUIUUSkJhSGUi0Foog0LYWhjIUCUUSaksJQxkqBKCJNR2Eo46FAFJGmojCU8VIgikjTUBjKzlAgikhTUBjKzlIgikjuKQylFhSIIpJrCkOpFQWiiOSWwlBqSYEoIrmkMJRaUyCKSO4oDKUeFIgikisKQ6kXBaKI5IbCUOpJgSgiuaAwlHpTIIpI5ikMpREUiCKSaQpDaRQFoohklsJQGkmBKCKZpDCURlMgikjmKAwlDQpEEckUhaGkRYEoIpmhMJQ0KRBFJBMUhpI2BaKIpE5hKFmgQBSRVCkMJSsUiCKSGoWhZIkCUURSoTCUrFEgikjDKQwlixSIItJQCkPJKgWiiDSMwlCyTIEoIg2hMJSsUyCKSN0pDCUPFIgiUlcKQ8kLBaKI1I3CUPJEgSgidaEwlLzJZCCa2XfM7HEzW5N2LSIydkNDQwpDyZ1MBiJwHTA/7SJEZOwGBwfp7e0FFIaSL5kMRHe/A3g67TpEZGx0mlTyzNw97RpKMrOZwK3ufnDC9AXAAoCurq6exYsXJ7a1ZcsWJk+eXI8y60L11lfe6oV81Dw0NDR8ZLhw4UJmzZqVbkFjkIf+jWu2eufNm7fS3ec0sKTS3D2TP8BMYE01r+3p6fFy+vr6yk7PGtVbX3mr1z37NQ8MDHhXV5d3dXX5wMBA5ustpnrrq1K9wArPQO5k8pSpiOSHTpNKs1Agisi4KQylmWQyEM1sEfBr4AAzW29mH0y7JhEZSWEozWZC2gWU4u5npF2DiCRTGEozyuQRoohkl8JQmpUCUUSqpjCUZqZAFJGqKAyl2SkQRaQihaG0AgWiiJSlMJRWoUAUkUQKQ2klCkQRKUlhKK1GgSgioygMpRUpEEVkBIWhtCoFoogMUxhKK1MgigigMBRRIIqIwlAEBaJIy1MYikQUiCItTGEo8jIFokiLUhiKjKRAFGlBCkOR0RSIIi1GYShSmgJRpIUoDEWSKRBFWoTCUKQ8BaJIC1AYilSmQBRpcgpDkeooEEWamMJQpHoKRJEmpTAUGRsFokgTUhiKjJ0CUaTJKAxFxkeBKNJEFIYi46dAFGkSCkORnaNAFGkCCkORnadAFMk5haFIbSgQRXJMYShSOwpEkZxSGIrUlgJRJIcUhiK1p0AUyRmFoUh9KBBFckRhKFI/E9IuQERKu+XeDVy+7AE2bt7K5w7dwZolv+CSj54OvByG8dfsPaWTeQfuRd/gE8OPP3PcAZxyWHfF9pPmBbh82QNs2LyVdjO2uw//a4CHtszAHaZ0dmAGH9h3K//3C7fhDn/aum1Eexf9cC2bt24DYI9JHRz02t2566Fn2O5Om8EuE9r487YdJWsqPI7X0x1r+/wl9/PCth2j3mtnRxuXnnrIcF8Uv/fPvHH7mD6PSn0r+WTuXvlVDWZm84ErgXbgGne/rNzr58yZ4ytWrEic3t/fP7xXnQdp1juejb5cvaUG3R/f/xjPvBANiFM6O7jopDcAJA7OUyZ1jBpYTzmsm1vu3cDFP1o73BZARxtsd9jh0G7G6/eaxINPPE98NT9n9ktcsXrC8GC+x6QOPv+OkTV0drSx9aUdI+brLuqPC29ZzaLfrBsOh4kT2vjLSy8PxvHAgCg0Oie0sTUM9vG2brl3w4igKPbeVz3CpRdfCMD+Z11O56v3GfG+k7S3GbvvMmG47+YduBe33vdY4nJq5ZzZL/Hl1SP3tzvaje3bndFxtfM62o1t28uPZW3AFacfCsB5S1azddvLIfiZQ7bTPaun7M5D8TydHe1ceursVEKx2cY0M1vp7nMaV1FpmTtCNLN24OvAW4H1wN1m9kN3/590K2t+xRv9hs1bOW/JaoBxbfSl2rvhrqERr9m8dRufunkV7WZs2+ElXxcf+As1rXj0aW66e92oQTB+cLDdnd8//nzJ2uJzPfPCNnpvWjVieqmjjA2bt/Kpm1eVDC6HEWFYvAyIjqAK7W7YvJXem1ZxwdLVvPPwbm767brh919s21Pr+NrVURi+5oxL+fPk1/LnKsIQYPsOH661VP83UqXAqnfbO4h2eIARwQaww53Llz2QuJ5fvuyBUfNs3ba97DySP2O6hmhmt5vZG+tVTHAE8KC7P+TuLwKLgZPrvEyh/EZfq/ZK2eEkhkEpW7dtZ9FvRodhI+xwanp09fyL27nhrqGyYbhp0XlAFIYdr5pes2W3oo2bt7Jx89bEaeXmG+s8kj9lT5ma2UHA+e7+3vD4cOBLwKPh+cdqXpDZu4D57v6h8Ph9wJHu/vGi1y0AFgB0dXX1LF68OLHNLVu2MHny5FqXWjdp1bt6w58Sp83ufmXitKR6y7WXpq5O+GMOxrE/blzP1xZGR4YXf+ESJrxqWsoVVS+rfTyxPToGeHH7yKP5rk545sU2DnjN7iXne2DTc6PmKbSXNE89NduYNm/evFycMv0Z8KbCA3e/BzjGzP438F9mtgT4F3ev5apvJZ4bldru/i3gWxBdQyx3frrZzrfXywWX/ZwNJfZ4u6d08on3JNeTVG9Se7VQuKFiPEpd38qa6Mjw5dOkE1712szXHJfPa4iHMzfh9OfmMtcQk+apJ41p9VHplOnbgIXxJ8zMgAeAbwKfAH4fjuJqZT0QPy80DdhYw/YlwWeOO4DOjvYRz3V2tA/fwVeL9kppM+hoK7UfVFpnRztnHDmdjvbq58mTrJ4mtdDd5T6qKZ0d7DGpA4huVprS2YER7VRd/q43csXphzKls2P49XtM6uDoffekPTTeZtEdoYV53nvUPnRP6Rz1GBiep9D2V08/lEkdpYe0zo42rjj9UE45rJtTDuvm0lNnj2i3e4/OstcCS82T1g01Uj9ldzndfTXwnsJjM/sl8HpgLXAXcBYwCJxtZm929wU1qOluYH8zex2wAXg38Hc1aFcqKGzctbq1vFR7tbzLdM6MPau7y/Tx50efYsiQjnbjiJl7cOcfnh4Vhu9/+9FccspsbvnpcrqntLNx81ZeGb7asPmFbaP6JulrCvHHxXeZTuns4A17v/zVh2KFzyi+HlS6G7m/v597E84q1DNEqm27EIwF/f39Y55Hms9Yz8F8BFjroy88fsLMBmpRkLu/ZGYfB5YRfe3iO+6+thZtS2W13uhLtXfJKbMTX7uzbSeJD+AT29v4ajhaKJ5WGNxXPPr08FcqCrrH+L24wldDioM+HkiFr3ycclg3Vy35BZ8683wADlnwFT7/vrcO1zils4M7Pzd3TP2TJKn/x0LhIM1oTIHo7mvKTH77TtYSX85PgJ/Uqj2R+ADe398/4rpPqcH9lMO66xYcpdodHBzkko+ezp67TdRfoBFJSc3+dJu7P1SrtkRaif4cm0g26G+ZiqRIYSiSHQpEkZQoDEWyRYEokgKFoUj2KBBFGkxhKJJNCkSRBlIYimSXAlGkQRSGItmmQBRpAIWhSPYpEEXqTGEokg8KRJE6UhiK5IcCUaROFIYi+aJAFKkDhaFI/igQRWpMYSiSTwpEkRpSGIrklwJRpEYUhiL5pkAUqQGFoUj+KRBFdpLCUKQ5KBBFdoLCUKR5KBBFxklhKNJcFIgi46AwFGk+CkSRMVIYijQnBaLIGCgMRZqXAlGkSgpDkeamQBSpgsJQpPkpEEUqUBiKtAYFokgZCkOR1qFAFEmgMBRpLQpEkRIUhiKtR4EoUkRhKNKaFIgiMQpDkdalQBQJFIYirU2BKILCUEQUiCIKQxEBFIjS4hSGIlKgQJSWpTAUkbjMBaKZnWZma81sh5nNSbseaU4KQxEplrlABNYApwJ3pF2INKehoSGFoYiMMiHtAoq5+wCAmaVdijShwcFBent7mThxosJQREYwd0+7hpLMrB/4tLuvSJi+AFgA0NXV1bN48eLEtrZs2cLkyZPrUWZdqN76GBoaore3F3fnyiuvZJ999km7pKrlpY8LVG99NVu98+bNW+nu6V8ic/eG/wC3E50aLf45OfaafmBONe319PR4OX19fWWnZ43qrb2BgQHv6uryrq4uv/7669MuZ8zy0Mdxqre+mq1eYIWnkEXFP6mcMnX3Y9NYrrSm4htoNm3alG5BIpJJWbypRqRmdDepiFQrc4FoZu80s/XAm4Afm9mytGuSfFIYishYZPEu06XA0rTrkHxTGIrIWGXuCFFkZykMRWQ8FIjSVBSGIjJeCkRpGgpDEdkZCkRpCgpDEdlZCkTJPYWhiNSCAlFyTWEoIrWiQJTcUhiKSC0pECWXFIYiUmsKRMkdhaGI1IMCUXJFYSgi9aJAlNxQGIpIPSkQJRcUhiJSbwpEyTyFoYg0ggJRMk1hKCKNokCUzFIYikgjKRAlkxSGItJoCkTJHIWhiKRBgSiZojAUkbQoECUzFIYikiYFomSCwlBE0qZAlNQpDEUkCxSIkiqFoYhkhQJRUqMwFJEsUSBKKhSGIpI1CkRpOIWhiGSRAlEaSmEoIlmlQJSGURiKSJYpEKUhFIYiknUKRKk7haGI5IECUepKYSgieaFAlLpRGIpInigQpS4UhiKSNwpEqTmFoYjkkQJRakphKCJ5pUCUmlEYikieZS4QzexyMxs0s/vNbKmZTUm7JqlMYSgieZe5QASWAwe7+yHA74DzUq5HKhgaGlIYikjuZS4Q3f02d38pPLwLmJZmPVLe4OAgvb29gMJQRPLN3D3tGhKZ2Y+Am9z9hhLTFgALALq6unoWL16c2M6WLVuYPHly3eqstbzUOzQ0RG9vL+7OlVdeyT777JN2SVXJS//G5a1m1VtfzVbvvHnzVrr7nAaWVJq7N/wHuB1YU+Ln5NhrLgCWEkK73E9PT4+X09fXV3Z61uSh3oGBAe/q6vKuri6//vrr0y5nTPLQv8XyVrPqra9mqxdY4SlkUfHPhJRC+Nhy083sTOBE4C2hsyRDim+g2bRpU7oFiYjUQOauIZrZfOBc4CR3fyHtemQk3U0qIs0qc4EIXAXsDiw3s1VmdnXaBUlEYSgizSyVU6bluPt+adcgoykMRaTZZfEIUTJGYSgirUCBKGUpDEWkVSgQJZHCUERaiQJRSlIYikirUSDKKApDEWlFCkQZ4eGHH1YYikhLUiDKCN3d3Zx44okKQxFpOZn7HqKka+LEiVxzzTVplyEi0nA6QhQREUGBKCIiAigQRUREAAWiiIgIoEAUEREBFIgiIiKAAlFERARQIIqIiABg7p52DTvNzJ4AHi3zkqnAkw0qpxZUb33lrV7IX82qt76ard4Z7r5Xo4pJ0hSBWImZrXD3OWnXUS3VW195qxfyV7PqrS/VWx86ZSoiIoICUUREBGidQPxW2gWMkeqtr7zVC/mrWfXWl+qtg5a4higiIlJJqxwhioiIlKVAFBERoQUD0cw+bWZuZlPTrqUcM/uimd1vZqvM7DYz2zvtmsoxs8vNbDDUvNTMpqRdUzlmdpqZrTWzHWaW2dvBzWy+mT1gZg+a2efSrqcSM/uOmT1uZmvSrqUSM5tuZn1mNhDWhbPTrqkcM9vVzH5rZveFei9Ou6ZqmFm7md1rZremXUslLRWIZjYdeCswlHYtVbjc3Q9x90OBW4F/SrmeSpYDB7v7IcDvgPNSrqeSNcCpwB1pF5LEzNqBrwPHAwcBZ5jZQelWVdF1wPy0i6jSS8A57j4LOAr4WMb79y/AMe7+RuBQYL6ZHZVuSVU5GxhIu4hqtFQgAl8BPgtk/k4id3829nA3Ml6zu9/m7i+Fh3cB09KspxJ3H3D3B9Kuo4IjgAfd/SF3fxFYDJycck1lufsdwNNp11ENd3/M3e8Jvz9HNGh3p1tVMo9sCQ87wk+mxwUzmwa8Hbgm7Vqq0TKBaGYnARvc/b60a6mWmS00s3XAe8j+EWLcB4Cfpl1EE+gG1sUeryfDA3aemdlM4DDgNymXUlY4/bgKeBxY7u6Zrhf4KtFByI6U66jKhLQLqCUzux14TYlJFwDnA29rbEXllavX3f/T3S8ALjCz84CPA59vaIFFKtUbXnMB0amo7zeytlKqqTfjrMRzmT4iyCMzmwz8AOgtOjOTOe6+HTg0XKNfamYHu3smr9ea2YnA4+6+0szmplxOVZoqEN392FLPm9ls4HXAfWYG0em8e8zsCHff1MASR0iqt4QbgR+TciBWqtfMzgROBN7iGfiC6xj6N6vWA9Njj6cBG1OqpSmZWQdRGH7f3ZekXU+13H2zmfUTXa/NZCACRwMnmdkJwK7AK8zsBnd/b8p1JWqJU6buvtrdX+3uM919JtFAc3iaYViJme0fe3gSMJhWLdUws/nAucBJ7v5C2vU0ibuB/c3sdWY2EXg38MOUa2oaFu0dXwsMuPsVaddTiZntVbh728w6gWPJ8Ljg7ue5+7Qw5r4b+HmWwxBaJBBz6jIzW2Nm9xOd6s30LeHAVcDuwPLwVZGr0y6oHDN7p5mtB94E/NjMlqVdU7Fwk9LHgWVEN3zc7O5r062qPDNbBPwaOMDM1pvZB9OuqYyjgfcBx4R1dlU4msmq1wJ9YUy4m+gaYua/ypAn+tNtIiIi6AhRREQEUCCKiIgACkQRERFAgSgiIgIoEEVERAAFooiICKBAFBERARSIIpkV/k/Ms2OPF5rZJ9OsSaSZ6Yv5IhkV/geGJe5+uJm1Ab8HjnD3p9KtTKQ5NdUf9xZpJu7+iJk9ZWaHAV3AvQpDkfpRIIpk2zXAWUT/jdV30i1FpLnplKlIhoX/5WI10f+Ovn/4//BEpA50hCiSYe7+opn1AZsVhiL1pUAUybBwM81RwGlp1yLS7PS1C5GMMrODgAeBn7n779OuR6TZ6RqiiIgIOkIUEREBFIgiIiKAAlFERARQIIqIiAAKRBEREQD+P4kjt3iMJLz0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "EPOCHS = 66\n",
    "LEARN_R = 3e-5\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation,forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n",
    "y_test_np = Var_to_nparray(y_test)\n",
    "output_test = forward(x_test, NN)\n",
    "\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Train loss:  {:4.3f}\".format(train_loss[-1]))\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 ( 0.00%) Train loss: 104.227 \t Validation loss: 106.711\n",
      " 100 ( 6.00%) Train loss: 97.145 \t Validation loss: 99.733\n",
      " 200 (12.00%) Train loss: 87.285 \t Validation loss: 89.700\n",
      " 300 (18.01%) Train loss: 71.732 \t Validation loss: 73.805\n",
      " 400 (24.01%) Train loss: 51.566 \t Validation loss: 53.004\n",
      " 500 (30.01%) Train loss: 32.824 \t Validation loss: 33.237\n",
      " 600 (36.01%) Train loss: 21.147 \t Validation loss: 20.346\n",
      " 700 (42.02%) Train loss: 16.150 \t Validation loss: 14.316\n",
      " 800 (48.02%) Train loss: 14.505 \t Validation loss: 11.995\n",
      " 900 (54.02%) Train loss: 14.005 \t Validation loss: 11.129\n",
      "1000 (60.02%) Train loss: 13.817 \t Validation loss: 10.768\n",
      "1100 (66.03%) Train loss: 13.711 \t Validation loss: 10.585\n",
      "1200 (72.03%) Train loss: 13.629 \t Validation loss: 10.470\n",
      "1300 (78.03%) Train loss: 13.562 \t Validation loss: 10.388\n",
      "1400 (84.03%) Train loss: 13.488 \t Validation loss: 10.319\n",
      "1500 (90.04%) Train loss: 13.413 \t Validation loss: 10.255\n",
      "1600 (96.04%) Train loss: 13.338 \t Validation loss: 10.192\n",
      "_______________________________________________________________\n",
      "Train loss:  13.289\n",
      "Test loss:  10.989\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "EPOCHS = 1666\n",
    "LEARN_R = 3e-5\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation,forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%100==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n",
    "y_test_np = Var_to_nparray(y_test)\n",
    "output_test = forward(x_test, NN)\n",
    "\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Train loss:  {:4.3f}\".format(train_loss[-1]))\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 ( 0.00%) Train loss: 105.586 \t Validation loss: 108.400\n",
      " 100 ( 3.75%) Train loss: 101.126 \t Validation loss: 103.975\n",
      " 200 ( 7.50%) Train loss: 95.937 \t Validation loss: 98.824\n",
      " 300 (11.25%) Train loss: 86.911 \t Validation loss: 89.824\n",
      " 400 (15.00%) Train loss: 72.418 \t Validation loss: 75.262\n",
      " 500 (18.75%) Train loss: 53.452 \t Validation loss: 55.947\n",
      " 600 (22.51%) Train loss: 35.429 \t Validation loss: 37.109\n",
      " 700 (26.26%) Train loss: 23.761 \t Validation loss: 24.269\n",
      " 800 (30.01%) Train loss: 18.494 \t Validation loss: 17.857\n",
      " 900 (33.76%) Train loss: 16.636 \t Validation loss: 15.164\n",
      "1000 (37.51%) Train loss: 16.010 \t Validation loss: 14.021\n",
      "1100 (41.26%) Train loss: 15.732 \t Validation loss: 13.436\n",
      "1200 (45.01%) Train loss: 15.546 \t Validation loss: 13.071\n",
      "1300 (48.76%) Train loss: 15.387 \t Validation loss: 12.799\n",
      "1400 (52.51%) Train loss: 15.245 \t Validation loss: 12.574\n",
      "1500 (56.26%) Train loss: 15.120 \t Validation loss: 12.382\n",
      "1600 (60.02%) Train loss: 15.004 \t Validation loss: 12.212\n",
      "1700 (63.77%) Train loss: 14.899 \t Validation loss: 12.056\n",
      "1800 (67.52%) Train loss: 14.797 \t Validation loss: 11.911\n",
      "1900 (71.27%) Train loss: 14.702 \t Validation loss: 11.777\n",
      "2000 (75.02%) Train loss: 14.614 \t Validation loss: 11.653\n",
      "2100 (78.77%) Train loss: 14.528 \t Validation loss: 11.535\n",
      "2200 (82.52%) Train loss: 14.443 \t Validation loss: 11.422\n",
      "2300 (86.27%) Train loss: 14.352 \t Validation loss: 11.315\n",
      "2400 (90.02%) Train loss: 14.254 \t Validation loss: 11.212\n",
      "2500 (93.77%) Train loss: 14.157 \t Validation loss: 11.110\n",
      "2600 (97.52%) Train loss: 14.059 \t Validation loss: 11.007\n",
      "_______________________________________________________________\n",
      "Train loss:  13.984\n",
      "Test loss:  12.772\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "EPOCHS = 2666\n",
    "LEARN_R = 3e-5\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation,forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%100==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n",
    "y_test_np = Var_to_nparray(y_test)\n",
    "output_test = forward(x_test, NN)\n",
    "\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Train loss:  {:4.3f}\".format(train_loss[-1]))\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYPZP-eTFtIo"
   },
   "source": [
    "# Next steps - classification\n",
    "\n",
    "It is straight forward to extend what we have done to classification. \n",
    "\n",
    "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
    "\n",
    "Next week we will see how to perform classification in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsVPul3QFtIo"
   },
   "source": [
    "## Exercise l) optional - Implement backpropagation for classification\n",
    "\n",
    "Should be possible with very few lines of code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC8QrI2tFtIp"
   },
   "outputs": [],
   "source": [
    "# Just add code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APqhJv3tta1O"
   },
   "source": [
    "## Exercise m) optional - Introduce a NeuralNetwork class\n",
    "\n",
    "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dqfnor1ouMLq"
   },
   "outputs": [],
   "source": [
    "# just add some code"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "U4057_ljNvWB",
    "p_8n_SKnIW2F",
    "oLrGJytZFtGm",
    "jpIZPBpNI0pO",
    "_79HOAXrFtHK",
    "mqeyab9qFtGs",
    "-XyXBD37FtHk",
    "SrwSJ2UWFtHu",
    "zTBAmjsAFtIk",
    "qsVPul3QFtIo",
    "APqhJv3tta1O"
   ],
   "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
